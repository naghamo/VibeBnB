{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b12007d4-7a99-46d0-a14b-0f7e3eb83f25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1.\n",
    "\n",
    "  After checking extreme values, most of them are bounded columns (percentages, rates...) so its not a problem. \n",
    "\n",
    "  Counts of reviews were pretty high, so i did log1p trasnform of them.\n",
    "\n",
    "  We do have problem with price columns (all of them...) they have extremely high values (in raw data too)\n",
    "  I thought it might be because of the currency of that country, but it is listed as USD and EUR in coolumn, and i casted the prices we use at the end to USD. \n",
    "  Im not sure what to do about it...\n",
    "\n",
    "2.\n",
    "\n",
    "  In terms of embeddings, i made a simple numerical feature embedding which i scaled with 0-1 scaler.\n",
    "\n",
    "  For text, I filter the columns we've selected (title, description, amenitites, details, description_items, anything else?), limit some of the feature's text, and concatanate them in a proper structure that i use. We can try different stuff here\n",
    "\n",
    "  I've tried using sentence transformers, and for now it was very slow, maybe there is a way to optimize it idk... i didnt manage to finish and see the results.\n",
    "  I tried tf-idf with 2048 dim, and, well, it did finish with embeddings. I'm pretty sure we'll need embeddings in much smaller space though.\n",
    "  Seems like there are some clusters (around giant blob) forming from plots at the end (using TSNE and PCA), but it might be our way of imputation or some other noise, it is hard to say what embedding space we get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a38a478f-59ba-47b1-9766-d0b13cd220a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install reverse_geocoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96ddd15a-97ad-4213-952c-25a8e1a9e23d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "project_root = Path().resolve().parents[0]\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "from airbnb_data_loader import load_airbnb_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df7783ab-a8db-400a-ad01-e0a223ecfab4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "    \n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fd93e9c-2ffd-4192-80f1-7eb6e15363b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "airbnb_data = load_airbnb_data(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba641504-d36c-4135-ad08-0bafe57de619",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(airbnb_data.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1c44e6e-d4b0-4837-aa2e-ac601dd52368",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from airbnb_data_loader import analyze_missing_values\n",
    "\n",
    "analyze_missing_values(airbnb_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "315ff34b-5391-4944-ac99-dd500346c9a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cef8cca-edea-448a-8dd9-a2060f0ac18a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql import DataFrame, functions as F\n",
    "# from pyspark.sql.types import DoubleType, IntegerType, FloatType, LongType, StringType\n",
    "# import pandas as pd\n",
    "# from typing import Optional, List\n",
    "\n",
    "# def analyze_distributions(\n",
    "#     df: DataFrame, \n",
    "#     numerical_cols: Optional[List[str]] = None,\n",
    "#     text_cols: Optional[List[str]] = None,\n",
    "#     iqr_multiplier: float = 1.5,\n",
    "#     show_samples: bool = True\n",
    "# ) -> pd.DataFrame:\n",
    "#     \"\"\"\n",
    "#     Optimized distribution analysis: 2-Pass Approach (1 Stats Job + 1 Outlier Job).\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # Auto-detect Numerical Columns\n",
    "#     if numerical_cols is None:\n",
    "#         numerical_cols = []\n",
    "#         for field in df.schema.fields:\n",
    "#             if isinstance(field.dataType, (DoubleType, IntegerType, FloatType, LongType)):\n",
    "#                 if not field.name.startswith('is_') and 'id' not in field.name.lower():\n",
    "#                     numerical_cols.append(field.name)\n",
    "#         print(f\"Auto-detected {len(numerical_cols)} numerical columns\")\n",
    "    \n",
    "#     # Auto-detect Text Columns\n",
    "#     if text_cols is None:\n",
    "#         text_cols = []\n",
    "#         for field in df.schema.fields:\n",
    "#             if isinstance(field.dataType, StringType):\n",
    "#                 if field.name not in ['addr_cc', 'currency', 'category', 'cancellation_policy']:\n",
    "#                     text_cols.append(field.name)\n",
    "#         print(f\"Auto-detected {len(text_cols)} text columns\")\n",
    "    \n",
    "#     # Create checking DataFrame with length columns\n",
    "#     len_cols = [F.length(F.col(c)).alias(f\"len_{c}\") for c in text_cols]\n",
    "#     df_check = df.select(*numerical_cols, *len_cols)\n",
    "    \n",
    "#     all_cols = numerical_cols + [f\"len_{c}\" for c in text_cols]\n",
    "#     total_count = df.count()\n",
    "    \n",
    "#     print(f\"\\nAnalyzing {len(all_cols)} features ({total_count:,} rows)...\")\n",
    "\n",
    "#     # PASS 1: Compute statistics\n",
    "#     print(\"Pass 1: Computing global statistics...\")\n",
    "#     stats_exprs = []\n",
    "#     for c in all_cols:\n",
    "#         stats_exprs.extend([\n",
    "#             F.min(c).alias(f\"{c}_min\"),\n",
    "#             F.max(c).alias(f\"{c}_max\"),\n",
    "#             F.avg(c).alias(f\"{c}_mean\"),\n",
    "#             F.stddev(c).alias(f\"{c}_std\"),\n",
    "#             F.percentile_approx(c, [0.25, 0.5, 0.75], 5000).alias(f\"{c}_qs\")\n",
    "#         ])\n",
    "    \n",
    "#     stats_row = df_check.agg(*stats_exprs).first()\n",
    "    \n",
    "#     # PASS 2: Count outliers (batched)\n",
    "#     print(\"Pass 2: Checking outlier counts...\")\n",
    "    \n",
    "#     outlier_exprs = []\n",
    "#     fences = {}\n",
    "    \n",
    "#     for c in all_cols:\n",
    "#         q25, median, q75 = stats_row[f\"{c}_qs\"]\n",
    "        \n",
    "#         # Handle edge case: zero IQR (all values same)\n",
    "#         iqr = q75 - q25\n",
    "#         if iqr == 0:\n",
    "#             iqr = 0.01  # Tiny value to avoid division by zero\n",
    "        \n",
    "#         lower = q25 - (iqr_multiplier * iqr)\n",
    "#         upper = q75 + (iqr_multiplier * iqr)\n",
    "        \n",
    "#         fences[c] = (lower, upper)\n",
    "        \n",
    "#         # Batch count outliers\n",
    "#         is_outlier = (F.col(c) > upper) | (F.col(c) < lower)\n",
    "#         outlier_exprs.append(\n",
    "#             F.sum(F.when(is_outlier, 1).otherwise(0)).alias(f\"{c}_outliers\")\n",
    "#         )\n",
    "\n",
    "#     outlier_counts = df_check.agg(*outlier_exprs).first()\n",
    "\n",
    "#     # BUILD REPORT\n",
    "#     results_data = []\n",
    "#     outlier_details = []\n",
    "\n",
    "#     for c in all_cols:\n",
    "#         mn, mx = stats_row[f\"{c}_min\"], stats_row[f\"{c}_max\"]\n",
    "#         avg, std = stats_row[f\"{c}_mean\"], stats_row[f\"{c}_std\"]\n",
    "#         q25, median, q75 = stats_row[f\"{c}_qs\"]\n",
    "#         lower, upper = fences[c]\n",
    "        \n",
    "#         o_count = outlier_counts[f\"{c}_outliers\"]\n",
    "#         o_pct = (o_count / total_count * 100) if total_count else 0\n",
    "        \n",
    "#         # Status\n",
    "#         status = \"âœ“ OK\"\n",
    "#         if o_pct > 5:\n",
    "#             status = f\"âš  {o_pct:.1f}%\"\n",
    "#             outlier_details.append({\n",
    "#                 'feature': c, 'count': o_count, 'pct': o_pct, \n",
    "#                 'lower': lower, 'upper': upper\n",
    "#             })\n",
    "#         elif o_pct > 0:\n",
    "#             status = f\"â€¢ {o_pct:.1f}%\"\n",
    "            \n",
    "#         label = c.replace(\"len_\", \"ðŸ“ \") if c.startswith(\"len_\") else c\n",
    "        \n",
    "#         results_data.append({\n",
    "#             \"Feature\": label,\n",
    "#             \"Min\": round(mn, 2) if mn else None,\n",
    "#             \"Q25\": round(q25, 2) if q25 else None,\n",
    "#             \"Median\": round(median, 2) if median else None,\n",
    "#             \"Mean\": round(avg, 2) if avg else None,\n",
    "#             \"Q75\": round(q75, 2) if q75 else None,\n",
    "#             \"Max\": round(mx, 2) if mx else None,\n",
    "#             \"Outliers\": status,\n",
    "#             \"Lower Fence\": round(lower, 2),\n",
    "#             \"Upper Fence\": round(upper, 2)\n",
    "#         })\n",
    "\n",
    "#     pdf_results = pd.DataFrame(results_data)\n",
    "    \n",
    "#     print(\"\\n\" + \"=\"*80)\n",
    "#     print(\"DISTRIBUTION SUMMARY\")\n",
    "#     print(\"=\"*80)\n",
    "#     display(pdf_results)\n",
    "\n",
    "#     # Show samples for worst offenders only\n",
    "#     if show_samples and outlier_details:\n",
    "#         print(\"\\n\" + \"=\"*80)\n",
    "#         print(\"OUTLIER SAMPLES (Top 5 Issues)\")\n",
    "#         print(\"=\"*80)\n",
    "        \n",
    "#         for detail in sorted(outlier_details, key=lambda x: x['pct'], reverse=True)[:5]:\n",
    "#             feat = detail['feature']\n",
    "#             print(f\"\\n{feat}: {detail['count']:,} outliers ({detail['pct']:.2f}%)\")\n",
    "#             print(f\"  Valid range: [{detail['lower']:.1f}, {detail['upper']:.1f}]\")\n",
    "            \n",
    "#             # Sample extreme values\n",
    "#             samples = df_check.filter(\n",
    "#                 (F.col(feat) > detail['upper']) | (F.col(feat) < detail['lower'])\n",
    "#             ).select(feat).limit(3)\n",
    "            \n",
    "#             print(\"  Samples:\")\n",
    "#             samples.show(truncate=False)\n",
    "    \n",
    "#     # SPECIFIC CHECKS\n",
    "#     print(\"\\n\" + \"=\"*80)\n",
    "#     print(\"SPECIFIC VALUE CHECKS\")\n",
    "#     print(\"=\"*80)\n",
    "    \n",
    "#     checks = []\n",
    "    \n",
    "#     if 'price_per_night' in numerical_cols:\n",
    "#         zero_price = df.filter(F.col(\"price_per_night\") == 0).count()\n",
    "#         checks.append(f\"Zero prices: {zero_price:,} ({zero_price/total_count*100:.2f}%)\")\n",
    "        \n",
    "#         extreme_price = df.filter(F.col(\"price_per_night\") > 10000).count()\n",
    "#         checks.append(f\"Extreme prices (>$10k): {extreme_price:,} ({extreme_price/total_count*100:.2f}%)\")\n",
    "    \n",
    "#     if 'guests' in numerical_cols:\n",
    "#         zero_guests = df.filter(F.col(\"guests\") == 0).count()\n",
    "#         checks.append(f\"Zero guest capacity: {zero_guests:,} ({zero_guests/total_count*100:.2f}%)\")\n",
    "    \n",
    "#     for check in checks:\n",
    "#         print(f\"  â€¢ {check}\")\n",
    "    \n",
    "#     print(\"=\"*80)\n",
    "    \n",
    "#     return pdf_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f4242d3-9714-423c-be91-30ee286a71d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame, functions as F\n",
    "from pyspark.sql.types import DoubleType, IntegerType, FloatType, LongType, StringType\n",
    "import pandas as pd\n",
    "from typing import Optional, List\n",
    "\n",
    "def analyze_distributions(\n",
    "    df: DataFrame,\n",
    "    numerical_cols: Optional[List[str]] = None,\n",
    "    text_cols: Optional[List[str]] = None,\n",
    "    iqr_multiplier: float = 1.5,\n",
    "    show_samples: bool = True,\n",
    "    skip_all_null_cols: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Optimized distribution analysis: 2-pass approach (1 stats job + 1 outlier job).\n",
    "    Robust to columns that are entirely null (percentile_approx returns None).\n",
    "    \"\"\"\n",
    "\n",
    "    # Auto-detect Numerical Columns\n",
    "    if numerical_cols is None:\n",
    "        numerical_cols = []\n",
    "        for field in df.schema.fields:\n",
    "            if isinstance(field.dataType, (DoubleType, IntegerType, FloatType, LongType)):\n",
    "                if not field.name.startswith(\"is_\") and \"id\" not in field.name.lower():\n",
    "                    numerical_cols.append(field.name)\n",
    "        print(f\"Auto-detected {len(numerical_cols)} numerical columns\")\n",
    "\n",
    "    # Auto-detect Text Columns\n",
    "    if text_cols is None:\n",
    "        text_cols = []\n",
    "        for field in df.schema.fields:\n",
    "            if isinstance(field.dataType, StringType):\n",
    "                if field.name not in [\"addr_cc\", \"currency\", \"category\", \"cancellation_policy\"]:\n",
    "                    text_cols.append(field.name)\n",
    "        print(f\"Auto-detected {len(text_cols)} text columns\")\n",
    "\n",
    "    # Create checking DataFrame with length columns\n",
    "    len_cols = [F.length(F.col(c)).alias(f\"len_{c}\") for c in text_cols]\n",
    "    df_check = df.select(*numerical_cols, *len_cols)\n",
    "\n",
    "    all_cols = numerical_cols + [f\"len_{c}\" for c in text_cols]\n",
    "    total_count = df_check.count()\n",
    "\n",
    "    print(f\"\\nAnalyzing {len(all_cols)} features ({total_count:,} rows)...\")\n",
    "\n",
    "    # PASS 1: Compute statistics (+ non-null counts)\n",
    "    print(\"Pass 1: Computing global statistics...\")\n",
    "    stats_exprs = []\n",
    "    for c in all_cols:\n",
    "        stats_exprs.extend([\n",
    "            F.count(F.col(c)).alias(f\"{c}_nn\"),\n",
    "            F.min(c).alias(f\"{c}_min\"),\n",
    "            F.max(c).alias(f\"{c}_max\"),\n",
    "            F.avg(c).alias(f\"{c}_mean\"),\n",
    "            F.stddev(c).alias(f\"{c}_std\"),\n",
    "            # If percentile_approx returns null (all values null), coalesce to [None, None, None]\n",
    "            F.coalesce(\n",
    "                F.percentile_approx(c, [0.25, 0.5, 0.75], 5000),\n",
    "                F.array(F.lit(None), F.lit(None), F.lit(None))\n",
    "            ).alias(f\"{c}_qs\")\n",
    "        ])\n",
    "\n",
    "    stats_row = df_check.agg(*stats_exprs).first()\n",
    "\n",
    "    # Decide which columns are actually analyzable\n",
    "    nn_by_col = {c: int(stats_row[f\"{c}_nn\"]) for c in all_cols}\n",
    "    valid_cols = [c for c in all_cols if nn_by_col[c] > 0]\n",
    "    null_only_cols = [c for c in all_cols if nn_by_col[c] == 0]\n",
    "\n",
    "    if null_only_cols:\n",
    "        msg = f\"Found {len(null_only_cols)} all-null columns: {null_only_cols}\"\n",
    "        if skip_all_null_cols:\n",
    "            print(msg + \" -> skipping\")\n",
    "        else:\n",
    "            print(msg + \" -> will include (outliers=0, stats=None)\")\n",
    "\n",
    "    cols_for_outliers = valid_cols if skip_all_null_cols else all_cols\n",
    "\n",
    "    # PASS 2: Count outliers (batched) only for valid cols\n",
    "    print(\"Pass 2: Checking outlier counts...\")\n",
    "    outlier_exprs = []\n",
    "    fences = {}\n",
    "\n",
    "    for c in cols_for_outliers:\n",
    "        q25, median, q75 = stats_row[f\"{c}_qs\"]\n",
    "\n",
    "        # If any quartile is None (should only happen when nn==0, but safe anyway)\n",
    "        if q25 is None or q75 is None:\n",
    "            fences[c] = (None, None)\n",
    "            outlier_exprs.append(F.lit(0).alias(f\"{c}_outliers\"))\n",
    "            continue\n",
    "\n",
    "        iqr = q75 - q25\n",
    "        if iqr == 0 or iqr is None:\n",
    "            iqr = 0.01  # tiny value to avoid 0-width fences\n",
    "\n",
    "        lower = q25 - (iqr_multiplier * iqr)\n",
    "        upper = q75 + (iqr_multiplier * iqr)\n",
    "        fences[c] = (lower, upper)\n",
    "\n",
    "        is_outlier = (F.col(c) > upper) | (F.col(c) < lower)\n",
    "        outlier_exprs.append(F.sum(F.when(is_outlier, 1).otherwise(0)).alias(f\"{c}_outliers\"))\n",
    "\n",
    "    outlier_counts = df_check.agg(*outlier_exprs).first() if outlier_exprs else None\n",
    "\n",
    "    # BUILD REPORT\n",
    "    results_data = []\n",
    "    outlier_details = []\n",
    "\n",
    "    cols_for_report = valid_cols if skip_all_null_cols else all_cols\n",
    "\n",
    "    for c in cols_for_report:\n",
    "        mn, mx = stats_row[f\"{c}_min\"], stats_row[f\"{c}_max\"]\n",
    "        avg, std = stats_row[f\"{c}_mean\"], stats_row[f\"{c}_std\"]\n",
    "        q25, median, q75 = stats_row[f\"{c}_qs\"]\n",
    "\n",
    "        lower, upper = fences.get(c, (None, None))\n",
    "\n",
    "        if outlier_counts is not None and f\"{c}_outliers\" in outlier_counts.asDict():\n",
    "            o_count = int(outlier_counts[f\"{c}_outliers\"])\n",
    "        else:\n",
    "            o_count = 0\n",
    "\n",
    "        o_pct = (o_count / total_count * 100) if total_count else 0.0\n",
    "\n",
    "        # Status\n",
    "        if nn_by_col[c] == 0:\n",
    "            status = \"âˆ… all null\"\n",
    "        else:\n",
    "            status = \"âœ“ OK\"\n",
    "            if o_pct > 5:\n",
    "                status = f\"âš  {o_pct:.1f}%\"\n",
    "                outlier_details.append({\n",
    "                    \"feature\": c, \"count\": o_count, \"pct\": o_pct,\n",
    "                    \"lower\": lower, \"upper\": upper\n",
    "                })\n",
    "            elif o_pct > 0:\n",
    "                status = f\"â€¢ {o_pct:.1f}%\"\n",
    "\n",
    "        label = c.replace(\"len_\", \"ðŸ“ \") if c.startswith(\"len_\") else c\n",
    "\n",
    "        # IMPORTANT: use `is not None` so 0 doesn't become None\n",
    "        results_data.append({\n",
    "            \"Feature\": label,\n",
    "            \"NonNull\": nn_by_col[c],\n",
    "            \"Min\": round(mn, 2) if mn is not None else None,\n",
    "            \"Q25\": round(q25, 2) if q25 is not None else None,\n",
    "            \"Median\": round(median, 2) if median is not None else None,\n",
    "            \"Mean\": round(avg, 2) if avg is not None else None,\n",
    "            \"Q75\": round(q75, 2) if q75 is not None else None,\n",
    "            \"Max\": round(mx, 2) if mx is not None else None,\n",
    "            \"Outliers\": status,\n",
    "            \"Lower Fence\": round(lower, 2) if lower is not None else None,\n",
    "            \"Upper Fence\": round(upper, 2) if upper is not None else None\n",
    "        })\n",
    "\n",
    "    pdf_results = pd.DataFrame(results_data)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"DISTRIBUTION SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "    display(pdf_results)\n",
    "\n",
    "    # Show samples for worst offenders only\n",
    "    if show_samples and outlier_details:\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"OUTLIER SAMPLES (Top 5 Issues)\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        for detail in sorted(outlier_details, key=lambda x: x[\"pct\"], reverse=True)[:5]:\n",
    "            feat = detail[\"feature\"]\n",
    "            print(f\"\\n{feat}: {detail['count']:,} outliers ({detail['pct']:.2f}%)\")\n",
    "            print(f\"  Valid range: [{detail['lower']:.1f}, {detail['upper']:.1f}]\")\n",
    "\n",
    "            samples = df_check.filter(\n",
    "                (F.col(feat) > detail[\"upper\"]) | (F.col(feat) < detail[\"lower\"])\n",
    "            ).select(feat).limit(3)\n",
    "\n",
    "            print(\"  Samples:\")\n",
    "            samples.show(truncate=False)\n",
    "\n",
    "    # SPECIFIC CHECKS\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"SPECIFIC VALUE CHECKS\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    checks = []\n",
    "\n",
    "    if numerical_cols and \"price_per_night\" in numerical_cols:\n",
    "        zero_price = df.filter(F.col(\"price_per_night\") == 0).count()\n",
    "        checks.append(f\"Zero prices: {zero_price:,} ({zero_price/total_count*100:.2f}%)\")\n",
    "\n",
    "        extreme_price = df.filter(F.col(\"price_per_night\") > 10000).count()\n",
    "        checks.append(f\"Extreme prices (>$10k): {extreme_price:,} ({extreme_price/total_count*100:.2f}%)\")\n",
    "\n",
    "    if numerical_cols and \"guests\" in numerical_cols:\n",
    "        zero_guests = df.filter(F.col(\"guests\") == 0).count()\n",
    "        checks.append(f\"Zero guest capacity: {zero_guests:,} ({zero_guests/total_count*100:.2f}%)\")\n",
    "\n",
    "    for check in checks:\n",
    "        print(f\"  â€¢ {check}\")\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    return pdf_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d369278-3437-4ba0-8239-1c412aa49f10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "results = analyze_distributions(\n",
    "    airbnb_data,\n",
    "    numerical_cols=None,\n",
    "    text_cols=None,\n",
    "    iqr_multiplier=1.5,\n",
    "    show_samples=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e169610e-2d44-4502-ae7c-f0bcb415909f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "results = analyze_distributions(\n",
    "    airbnb_data,\n",
    "    numerical_cols=[\"price_per_night\"],\n",
    "    text_cols=[],\n",
    "    iqr_multiplier=1.5,\n",
    "    show_samples=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5c6049c-a713-4779-b02d-6af918b03c48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "total_rows = airbnb_data.count()\n",
    "\n",
    "above_k = airbnb_data.filter(F.col(\"price_per_night\") > 1300).count()\n",
    "\n",
    "pct_above_k = (above_k / total_rows * 100) if total_rows else 0\n",
    "\n",
    "print(f\"Total rows: {total_rows:,}\")\n",
    "print(f\"price_per_night > k: {above_k:,}\")\n",
    "print(f\"Percentage: {pct_above_k:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12f9af16-845e-466a-9eb8-b61e3f350d9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "total_rows = airbnb_data.count()\n",
    "\n",
    "above_1000 = airbnb_data.filter(F.col(\"price_per_night\") > 1000).count()\n",
    "\n",
    "pct_above_1000 = (above_1000 / total_rows * 100) if total_rows else 0\n",
    "\n",
    "print(f\"Total rows: {total_rows:,}\")\n",
    "print(f\"price_per_night > 1000: {above_1000:,}\")\n",
    "print(f\"Percentage: {pct_above_1000:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a250cb0-53c7-41fb-b6fd-975ff66ce765",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField\n",
    "\n",
    "# Check original raw airbnb to see what the fuck is up\n",
    "\n",
    "from airbnb_data_loader import load_raw_airbnb_data, select_relevant_columns, cast_and_clean_types\n",
    "raw_airbnb = load_raw_airbnb_data(spark)\n",
    "\n",
    "casted_airbnb = select_relevant_columns(raw_airbnb)\n",
    "init_clean_airbnb = cast_and_clean_types(casted_airbnb)\n",
    "\n",
    "pricing_schema = StructType([\n",
    "    StructField(\"airbnb_service_fee\", DoubleType()),\n",
    "    StructField(\"cleaning_fee\", DoubleType()),\n",
    "    StructField(\"initial_price_per_night\", DoubleType()),\n",
    "    StructField(\"num_of_nights\", IntegerType()),\n",
    "    StructField(\"price_per_night\", DoubleType()),\n",
    "    StructField(\"price_without_fees\", DoubleType()),\n",
    "    StructField(\"special_offer\", DoubleType()),\n",
    "    StructField(\"taxes\", DoubleType())\n",
    "])\n",
    "\n",
    "raw_parsed = raw_airbnb.withColumn(\n",
    "    \"pricing_struct\", \n",
    "    F.from_json(F.col(\"pricing_details\"), pricing_schema)\n",
    ").select(\n",
    "    \"*\",\n",
    "    \"pricing_struct.*\"\n",
    ").withColumn(\n",
    "    \"total_price\", \n",
    "    F.col(\"total_price\").cast(\"double\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "207c0436-e39c-4656-8752-c846d828fe3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "results = analyze_distributions(\n",
    "    raw_parsed,\n",
    "    numerical_cols=[\"total_price\", \n",
    "                    \"airbnb_service_fee\",\n",
    "                    \"cleaning_fee\",\n",
    "                    \"initial_price_per_night\",\n",
    "                    \"num_of_nights\", \n",
    "                    \"price_per_night\", \n",
    "                    \"price_without_fees\",\n",
    "                    \"special_offer\", \n",
    "                    \"taxes\"\n",
    "        ],\n",
    "    text_cols=[],\n",
    "    iqr_multiplier=1.5,\n",
    "    show_samples=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "980d23a1-78b7-4651-875d-510c9c445de6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "results = analyze_distributions(\n",
    "    init_clean_airbnb,\n",
    "    numerical_cols=None,\n",
    "    text_cols=[],\n",
    "    iqr_multiplier=1.5,\n",
    "    show_samples=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9a8dc7e-8550-4cc5-bc4e-a87998523ec5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "print(\"Applying Log1p transformation to problematic counts...\")\n",
    "\n",
    "# 1. Apply Log(1+x) Transformation\n",
    "# coalesce(col, 0) is redundant in our case since we imputed the vals\n",
    "airbnb_data = airbnb_data.withColumn(\n",
    "    \"log_property_number_of_reviews\", \n",
    "    F.log1p(F.coalesce(F.col(\"property_number_of_reviews\"), F.lit(0)))\n",
    ").withColumn(\n",
    "    \"log_host_number_of_reviews\", \n",
    "    F.log1p(F.coalesce(F.col(\"host_number_of_reviews\"), F.lit(0)))\n",
    ")\n",
    "\n",
    "# I haven't handled the thing with prices as of now ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "885c61b0-6b7a-4164-81b5-fc3341161ab6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Trying out embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4b13329-f864-4ecd-8670-be191c396968",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import udf, col, concat_ws, coalesce, lit, pandas_udf\n",
    "from pyspark.sql.types import StringType, ArrayType, FloatType\n",
    "from pyspark.ml.feature import VectorAssembler, MinMaxScaler\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4a662f0-59b8-4156-b4e2-747615a661f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "numerical_cols = [\n",
    "    'ratings', 'log_property_number_of_reviews',\n",
    "    'host_rating', 'log_host_number_of_reviews', 'host_response_rate',\n",
    "    'hosts_year', 'is_supperhost', 'is_guest_favorite',\n",
    "    'rating_cleanliness', 'rating_accuracy', 'rating_checkin',\n",
    "    'rating_communication', 'rating_location', 'rating_value', 'guests', 'is_available'\n",
    "]\n",
    "# + ['price_per_night'] when it it is good\n",
    "\n",
    "print(f\"Scaling {len(numerical_cols)} numerical features...\")\n",
    "\n",
    "assembler = VectorAssembler(inputCols=numerical_cols, outputCol=\"num_vec_raw\")\n",
    "df_assembled = assembler.transform(airbnb_data)\n",
    "\n",
    "scaler = MinMaxScaler(inputCol=\"num_vec_raw\", outputCol=\"num_vec_scaled\")\n",
    "scaler_model = scaler.fit(df_assembled)\n",
    "df_scaled = scaler_model.transform(df_assembled)\n",
    "\n",
    "@udf(ArrayType(FloatType()))\n",
    "def vector_to_array(v):\n",
    "    return v.toArray().astype(float).tolist()\n",
    "\n",
    "df_num = df_scaled.withColumn(\n",
    "    \"numerical_features\", \n",
    "    vector_to_array(col(\"num_vec_scaled\"))\n",
    ").drop(\"num_vec_raw\", \"num_vec_scaled\")\n",
    "\n",
    "print(\"Numerical features ready\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5dc682bb-f802-45c1-80d1-2a39ab3badea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from typing import Iterator\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import pandas_udf, col\n",
    "from pyspark.sql.types import ArrayType, FloatType, StringType\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# ===================================================\n",
    "# Its pretty slow it hasnt finished in an hour so idk\n",
    "# ===================================================\n",
    "\n",
    "# # ITERATOR PATTERN for embedding\n",
    "# @pandas_udf(ArrayType(FloatType()))\n",
    "# def embed_text_optimized(iterator: Iterator[pd.Series]) -> Iterator[pd.Series]:\n",
    "#     \"\"\"Load model once per partition, process in batches\"\"\"\n",
    "#     model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    \n",
    "#     for batch in iterator:\n",
    "#         if batch.empty:\n",
    "#             yield pd.Series([], dtype=object)\n",
    "#             continue\n",
    "        \n",
    "#         texts = batch.fillna(\"\").tolist()\n",
    "#         embeddings = model.encode(\n",
    "#             texts,\n",
    "#             batch_size=128,\n",
    "#             show_progress_bar=False,\n",
    "#             convert_to_numpy=True,\n",
    "#             normalize_embeddings=True\n",
    "#         )\n",
    "#         yield pd.Series(embeddings.tolist())\n",
    "\n",
    "# @udf(ArrayType(FloatType()))\n",
    "# def normalize_embedding(emb):\n",
    "#     if not emb:\n",
    "#         return emb\n",
    "#     emb_array = np.array(emb)\n",
    "#     norm = np.linalg.norm(emb_array)\n",
    "#     if norm == 0:\n",
    "#         return emb\n",
    "#     return (emb_array / norm).astype(float).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cdeda67-1d0b-4197-804b-7e12e7f432d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf\n",
    "from pyspark.sql.types import StringType\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Extract amenity text using Pandas UDF\n",
    "@pandas_udf(StringType())\n",
    "def extract_features_structs(amenities: pd.Series, desc_items: pd.Series, details: pd.Series) -> pd.Series:\n",
    "    import json\n",
    "    \n",
    "    # Define limits for each section to prevent bloating\n",
    "    AMENITY_LIMIT = 40  # Amenities can be huge, cap at 40\n",
    "    DESC_LIMIT = 10     # Usually small, but safety cap\n",
    "    DETAILS_LIMIT = 10  # Usually small, but safety cap\n",
    "\n",
    "    def extract_single(amenities_json, description_items, details_json):\n",
    "        parts = []\n",
    "        \n",
    "        # A. Parse Amenities (The messy part)\n",
    "        if amenities_json:\n",
    "            try:\n",
    "                data = json.loads(amenities_json)\n",
    "                amenity_count = 0  # Counter for amenities\n",
    "                \n",
    "                for group in data:\n",
    "                    if amenity_count >= AMENITY_LIMIT: break # Stop group loop if limit reached\n",
    "\n",
    "                    group_name = group.get('group_name')\n",
    "                    \n",
    "                    for item in group.get('items', []):\n",
    "                        if amenity_count >= AMENITY_LIMIT: break # Stop item loop if limit reached\n",
    "                        \n",
    "                        name = item.get('name', '').strip()\n",
    "                        val = str(item.get('value', '')).strip()\n",
    "                        \n",
    "                        # Removing the entries with SYSTEM_ in them (idk what to do with them and amenities is huge btw)\n",
    "                        is_system_val = \"SYSTEM_\" in val\n",
    "                        \n",
    "                        if name and not is_system_val:\n",
    "                            # Case: \"Bed linens\" + \"Cotton\" -> \"Bed linens: cotton\"\n",
    "                            if val and val.lower() not in ['true', '1']:\n",
    "                                parts.append(f\"{name}: {val}\")\n",
    "                            else:\n",
    "                                parts.append(name)\n",
    "                            amenity_count += 1\n",
    "                        elif name and is_system_val:\n",
    "                            # Case: \"Hair dryer\" + \"SYSTEM_HAIRDRYER\" -> Just \"Hair dryer\"\n",
    "                            parts.append(name)\n",
    "                            amenity_count += 1\n",
    "                        elif not name and is_system_val:\n",
    "                            # Case: No name, just \"SYSTEM_WASHER\" -> \"Washer\"\n",
    "                            clean_val = val.replace(\"SYSTEM_\", \"\").replace(\"_\", \" \").lower()\n",
    "                            parts.append(clean_val)\n",
    "                            amenity_count += 1\n",
    "                            \n",
    "            except: pass\n",
    "\n",
    "        # Parse Description Items (e.g. \"Entire cabin\")\n",
    "        if description_items:\n",
    "            try:\n",
    "                items = json.loads(description_items)\n",
    "                # Take only up to DESC_LIMIT items\n",
    "                if items: parts.extend([str(x) for x in items[:DESC_LIMIT] if x])\n",
    "            except: pass\n",
    "            \n",
    "        # Parse Details (e.g. \"4 guests\")\n",
    "        if details_json:\n",
    "            try:\n",
    "                items = json.loads(details_json)\n",
    "                # Take only up to DETAILS_LIMIT items\n",
    "                if items: parts.extend([str(x) for x in items[:DETAILS_LIMIT] if x])\n",
    "            except: pass\n",
    "\n",
    "        # Deduplicate & Join\n",
    "        # dict.fromkeys preserves order (unlike set)\n",
    "        unique_parts = list(dict.fromkeys([p for p in parts if p and len(p) > 2]))\n",
    "        \n",
    "        # Return comma-separated\n",
    "        return \", \".join(unique_parts)\n",
    "\n",
    "    return pd.Series([\n",
    "        extract_single(a, d, det) \n",
    "        for a, d, det in zip(amenities.fillna(\"\"), desc_items.fillna(\"\"), details.fillna(\"\"))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1dc93af-4f58-4a21-a708-310ec459d84c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf, col, lit, concat, substring, coalesce\n",
    "from pyspark.sql.types import StringType\n",
    "import pandas as pd\n",
    "\n",
    "print(\"Extracting and cleaning text...\")\n",
    "\n",
    "# Step 1: Extract JSON to text\n",
    "df_text = df_num.withColumn(\n",
    "    \"features_text\",\n",
    "    extract_features_structs(col(\"amenities\"), col(\"description_items\"), col(\"details\"))\n",
    ")\n",
    "\n",
    "# We truncate description to 1000 chars to ensure features aren't cut off\n",
    "safe_desc = substring(col(\"description\"), 0, 1000)\n",
    "\n",
    "df_text = df_text.withColumn(\n",
    "    \"all_text_final\",\n",
    "    concat(\n",
    "        lit(\"Title: \"), coalesce(col(\"listing_title\"), lit(\"\")), lit(\". \"),\n",
    "        lit(\"Features: \"), coalesce(col(\"features_text\"), lit(\"\")), lit(\". \"),\n",
    "        lit(\"Description: \"), coalesce(safe_desc, lit(\"\")), lit(\".\")\n",
    "    )\n",
    ").withColumn(\n",
    "    \"all_text_clean\",\n",
    "    # Native Spark cleaning is faster than Python UDF for simple regex\n",
    "    F.lower(F.regexp_replace(col(\"all_text_final\"), r\"\\s+\", \" \"))\n",
    ").drop(\"features_text\", \"all_text_final\") # Dropping intermediate cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0491df1-e887-49d5-9067-69fc5a3b90d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_text.select(\"property_id\", \"all_text_clean\").limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b33525e4-344f-4846-a955-32aaf5fa37a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# print(\"Generating text embeddings...\")\n",
    "# df_embedded = df_text.withColumn(\n",
    "#     \"text_embedding\",\n",
    "#     embed_text_optimized(col(\"all_text_clean\"))\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa4dfa2e-aefe-4171-99d8-2bfd54c7fd93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, Normalizer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "print(\"Starting TF-IDF Vectorization...\")\n",
    "\n",
    "# 1. Tokenize: Split \"Nice place\" -> [\"nice\", \"place\"]\n",
    "tokenizer = Tokenizer(inputCol=\"all_text_clean\", outputCol=\"words\")\n",
    "\n",
    "# 2. Remove Stop Words: Remove \"the\", \"and\", \"is\" (English default)\n",
    "# This reduces noise significantly.\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
    "\n",
    "# 3. Term Frequency (HashingTF): Count word occurrences\n",
    "hashingTF = HashingTF(inputCol=\"filtered_words\", outputCol=\"raw_features\", numFeatures=2048)\n",
    "\n",
    "# 4. IDF: Down-weight common words (\"apartment\") and up-weight rare ones (\"sauna\")\n",
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"idf_features\")\n",
    "\n",
    "# 5. Normalize: Scale vector to unit length (L2)\n",
    "# Crucial so that long descriptions don't dominate short ones.\n",
    "normalizer = Normalizer(inputCol=\"idf_features\", outputCol=\"text_embedding\", p=2.0)\n",
    "\n",
    "# 6. Build and Run Pipeline\n",
    "pipeline = Pipeline(stages=[tokenizer, remover, hashingTF, idf, normalizer])\n",
    "\n",
    "# Fit the IDF model (scans data once to count doc frequencies)\n",
    "model = pipeline.fit(df_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5637ca66-9048-4ce8-b114-62a760ff0037",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Transform the data (creates the embeddings)\n",
    "df_tfidf = model.transform(df_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89fe752b-00cf-48ab-a736-e00e71e018d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 7. Clean up intermediate columns to keep DataFrame light\n",
    "df_embedded = df_tfidf.drop(\"words\", \"filtered_words\", \"raw_features\", \"idf_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "269d4009-3261-4539-a5ac-eab93649a611",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# sample_emb = df_embedded.select(\"text_embedding\").first()[0]\n",
    "# print(f\"Embedding dim: {len(sample_emb)}, norm: {np.linalg.norm(sample_emb):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f33384d0-34ca-4885-b8b4-1218573eaab6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_sample = df_embedded.select(\"text_embedding\").first()[0]\n",
    "print(f\"Final text embedding dimension: {len(final_sample)}\")\n",
    "print(f\"Final text embedding norm: {np.linalg.norm(final_sample):.6f}\")\n",
    "print(f\"Value range: [{min(final_sample):.4f}, {max(final_sample):.4f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a28967d7-1349-4953-86a9-9d7d3e8e8257",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1767296972276}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_embedded.select(\"property_id\", \"listing_name\", \"addr_cc\", \"numerical_features\", \"text_embedding\").limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c510ff0-c70d-4d91-a984-c493bab3a6ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "null_count = df_embedded.filter(\n",
    "    F.col(\"numerical_features\").isNull() |\n",
    "    F.col(\"text_embedding\").isNull()\n",
    ").count()\n",
    "\n",
    "print(\"\\nData loading completed\")\n",
    "print(f\"Rows with nulls: {null_count:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "525f829d-8eb8-4910-8065-32a29ddd7b45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Visualising stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fcd1365-0ffb-43ec-95d9-659ff9a44c02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "SAMPLE_SIZE = 3000\n",
    "EMBEDDING_COLS = [\"text_embedding\", \"numerical_features\"] \n",
    "\n",
    "# If you created the final combined one, add it too:\n",
    "# EMBEDDING_COLS.append(\"property_embedding\") \n",
    "\n",
    "print(f\"Sampling {SAMPLE_SIZE} rows from Spark...\")\n",
    "pdf_sample = df_embedded.sample(False, 0.1, seed=42).limit(SAMPLE_SIZE).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbac6e17-7812-49a0-826c-50aa12b49e79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pdf_sample.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd52bc5f-30cf-447d-86c4-1f0fb6919aee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from pyspark.ml.linalg import Vector, DenseVector, SparseVector\n",
    "\n",
    "def plot_embeddings_local(\n",
    "    pdf, \n",
    "    embedding_col=\"text_embedding\", \n",
    "    color_col=None,\n",
    "    hover_cols=[\"listing_name\"],\n",
    "    title=\"Embedding Clusters\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Pure Pandas visualization. Assumes 'pdf' is already computed.\n",
    "    - If color_col is None: Plots all points in one color.\n",
    "    - If color_col is Set: Colors points by that column (numeric or categorical).\n",
    "    \"\"\"\n",
    "    print(\"1. Parsing Vectors...\")\n",
    "    \n",
    "    # Robust parser for Spark Vectors inside Pandas\n",
    "    def parse_vector(obj):\n",
    "        if isinstance(obj, (DenseVector, SparseVector, Vector)):\n",
    "            return obj.toArray()\n",
    "        elif isinstance(obj, (list, np.ndarray)):\n",
    "            return np.array(obj)\n",
    "        return np.zeros(1)\n",
    "\n",
    "    # Stack into a proper numpy matrix\n",
    "    matrix = np.stack(pdf[embedding_col].apply(parse_vector).values)\n",
    "    \n",
    "    # 2. PCA (Global)\n",
    "    print(\"2. Running PCA...\")\n",
    "    pca = PCA(n_components=2)\n",
    "    pca_res = pca.fit_transform(matrix)\n",
    "    \n",
    "    pdf['pca_x'] = pca_res[:, 0]\n",
    "    pdf['pca_y'] = pca_res[:, 1]\n",
    "    \n",
    "    # 3. t-SNE (Local)\n",
    "    print(\"3. Running t-SNE...\")\n",
    "    if matrix.shape[1] > 50:\n",
    "        pca_50 = PCA(n_components=50)\n",
    "        matrix_50 = pca_50.fit_transform(matrix)\n",
    "    else:\n",
    "        matrix_50 = matrix\n",
    "        \n",
    "    tsne = TSNE(n_components=2, verbose=0, perplexity=30, init='pca', learning_rate='auto')\n",
    "    tsne_res = tsne.fit_transform(matrix_50)\n",
    "    \n",
    "    pdf['tsne_x'] = tsne_res[:, 0]\n",
    "    pdf['tsne_y'] = tsne_res[:, 1]\n",
    "    \n",
    "    # 4. Plotting\n",
    "    print(\"4. Generating Interactive Plot...\")\n",
    "    \n",
    "    fig = make_subplots(\n",
    "        rows=1, cols=2, \n",
    "        subplot_titles=(\"PCA (Global)\", \"t-SNE (Local)\"),\n",
    "        horizontal_spacing=0.1\n",
    "    )\n",
    "    \n",
    "    if color_col:\n",
    "        is_numeric = pd.api.types.is_numeric_dtype(pdf[color_col])\n",
    "        color_scale = \"Viridis\" if is_numeric else \"Turbo\"\n",
    "        color_arg = color_col\n",
    "    else:\n",
    "        # Default single color mode\n",
    "        is_numeric = False\n",
    "        color_scale = None\n",
    "        color_arg = None # Plotly handles None by using a default color\n",
    "\n",
    "    # Helper to add trace\n",
    "    def add_trace(x, y, row, col, show_legend):\n",
    "        # We use a trick here: if color_arg is None, update_traces handles the color\n",
    "        scatter = px.scatter(\n",
    "            pdf, x=x, y=y, \n",
    "            color=color_arg,\n",
    "            hover_data=hover_cols,\n",
    "            color_continuous_scale=color_scale\n",
    "        )\n",
    "        \n",
    "        # Apply single color if needed\n",
    "        if color_col is None:\n",
    "            scatter.update_traces(marker=dict(color='#636EFA')) # Standard Plotly Blue\n",
    "\n",
    "        for trace in scatter.data:\n",
    "            trace.showlegend = show_legend\n",
    "            \n",
    "            # Smart Hover Template\n",
    "            hover_lines = [f\"<b>{c}:</b> %{{customdata[{i}]}}\" for i, c in enumerate(hover_cols)]\n",
    "            \n",
    "            # Only add color tooltip if we actually colored by something\n",
    "            if color_col:\n",
    "                if is_numeric:\n",
    "                    hover_lines.append(f\"<b>{color_col}:</b> %{{marker.color:.2f}}\")\n",
    "                else:\n",
    "                    hover_lines.append(f\"<b>{color_col}:</b> %{{marker.color}}\")\n",
    "                \n",
    "            trace.hovertemplate = \"<br>\".join(hover_lines)\n",
    "            fig.add_trace(trace, row=row, col=col)\n",
    "\n",
    "    add_trace('pca_x', 'pca_y', 1, 1, False)\n",
    "    add_trace('tsne_x', 'tsne_y', 1, 2, True)\n",
    "\n",
    "    title_text = f\"{title} (Colored by {color_col})\" if color_col else f\"{title}\"\n",
    "\n",
    "    fig.update_layout(\n",
    "        title_text=title_text,\n",
    "        height=600, width=1200, template=\"plotly_white\"\n",
    "    )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7059c147-487c-4de6-a997-cd2962667b19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plot_embeddings_local(\n",
    "    pdf_sample, \n",
    "    embedding_col=\"text_embedding\", \n",
    "    color_col=None,\n",
    "    hover_cols=[\"listing_name\", \"price_per_night\"],\n",
    "    title=\"Airbnb Listings Analysis\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5d728a9-8b85-4430-9c57-adab248f346d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plot_embeddings_local(\n",
    "    pdf_sample, \n",
    "    embedding_col=\"numerical_features\", \n",
    "    color_col=None,\n",
    "    hover_cols=[\"listing_name\", \"price_per_night\"],\n",
    "    title=\"Airbnb Listings Analysis\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "airbnb_embeddings_eda",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
