{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e12c6fbc-6675-4272-ba0f-50638df5fa80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install reverse_geocoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7953d29-e73e-4658-9b4a-b98cd734bec4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "osm_base_dir = \"file:/Workspace/Users/nagham.omar@campus.technion.ac.il/VibeBnB/data/osm_pois\"\n",
    "osm_paths = {\n",
    "        \"Africa\": f\"{osm_base_dir}/africa_pois_enriched.parquet\",\n",
    "        \"Antarctica\": f\"{osm_base_dir}/antarctica_pois_enriched.parquet\",\n",
    "        \"Asia\": f\"{osm_base_dir}/asia_pois_enriched.parquet\",\n",
    "        \"Australia/Oceania\": f\"{osm_base_dir}/australia_oceania_pois_enriched.parquet\",\n",
    "        \"Central America\": f\"{osm_base_dir}/central_america_pois_enriched.parquet\",\n",
    "        \"Europe\": f\"{osm_base_dir}/europe_pois_enriched.parquet\",\n",
    "        \"North America\": f\"{osm_base_dir}/north_america_pois_enriched.parquet\",\n",
    "        \"South America\": f\"{osm_base_dir}/south_america_pois_enriched.parquet\",\n",
    "    }\n",
    "def build_continents_from_osm(spark: SparkSession, osm_paths: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Build {continent: [country_codes]} from OSM parquet files.\n",
    "    \"\"\"\n",
    "    continents = {}\n",
    "\n",
    "    for continent, path in osm_paths.items():\n",
    "        df = spark.read.parquet(path)\n",
    "\n",
    "        if \"addr_cc\" not in df.columns:\n",
    "            continents[continent] = []\n",
    "            continue\n",
    "\n",
    "        countries = (\n",
    "            df.select(\"addr_cc\")\n",
    "              .where(F.col(\"addr_cc\").isNotNull())\n",
    "              .distinct()\n",
    "              .orderBy(\"addr_cc\")\n",
    "              .rdd.flatMap(lambda x: x)\n",
    "              .collect()\n",
    "        )\n",
    "\n",
    "        continents[continent] = countries\n",
    "\n",
    "    return continents\n",
    "continents = build_continents_from_osm(spark, osm_paths)\n",
    "\n",
    "for k, v in continents.items():\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdaa6900-a4a2-4300-8c0a-e7415032d171",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{\"final_url\":{\"format\":{\"preset\":\"string-preset-url\",\"locale\":\"en\"}}}},\"syncTimestamp\":1767632345120}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# data_join.py\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "from config import *\n",
    "from airbnb_data_loader import load_airbnb_data\n",
    "\n",
    "def norm(c):\n",
    "    \"\"\"Normalize text fields for joins (lowercase, trim, remove punctuation).\"\"\"\n",
    "    return F.lower(F.regexp_replace(F.trim(F.coalesce(c, F.lit(\"\"))), r\"[\\.\\,\\-\\(\\)\\[\\]]\", \"\"))\n",
    "\n",
    "\n",
    "def cities_airbnb_join(cities_df: DataFrame, airbnb_df: DataFrame) -> DataFrame:\n",
    "    \"\"\"Join Airbnb listings with cities metadata using normalized city names.\"\"\"\n",
    "    return (\n",
    "        airbnb_df\n",
    "        .withColumn(\"city_key\", norm(F.col(\"addr_name\")))\n",
    "        .withColumn(\"region_key\", norm(F.col(\"addr_admin1\")))\n",
    "        .withColumn(\"country_key\", norm(F.col(\"addr_cc\")))\n",
    "        .alias(\"a\")\n",
    "        .join(\n",
    "            F.broadcast(\n",
    "                cities_df\n",
    "                .withColumn(\"city_key\", norm(F.col(\"city\")))\n",
    "                .withColumn(\"region_key\", norm(F.col(\"region\")))\n",
    "                .withColumn(\"country_key\", norm(F.col(\"country\")))\n",
    "                .alias(\"c\")\n",
    "            ),\n",
    "            on=[F.col(\"a.city_key\") == F.col(\"c.city_key\")],\n",
    "            how=\"left\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def _sanitize(c: str) -> str:\n",
    "    \"\"\"Make column-safe names for env features.\"\"\"\n",
    "    return \"\".join(ch if ch.isalnum() else \"_\" for ch in c)\n",
    "\n",
    "\n",
    "def osm_join_one_continent(continent: str, airbnb_df: DataFrame, osm_df: DataFrame) -> DataFrame:\n",
    "    \"\"\"Compute per-listing environment counts from OSM for one continent.\"\"\"\n",
    "    airbnb_scope = (\n",
    "        airbnb_df\n",
    "        .filter(F.col(\"addr_cc\").isin(continents[continent]))\n",
    "        .select(\"property_id\", F.col(\"lat\").cast(\"double\").alias(\"lat\"), F.col(\"long\").cast(\"double\").alias(\"lon\"), \"addr_cc\")\n",
    "        .filter(F.col(\"lat\").isNotNull() & F.col(\"lon\").isNotNull())\n",
    "        .dropDuplicates([\"property_id\"])\n",
    "    )\n",
    "\n",
    "    osm_geo = (\n",
    "        osm_df\n",
    "        .select(F.col(\"lat\").cast(\"double\").alias(\"p_lat\"), F.col(\"lon\").cast(\"double\").alias(\"p_lon\"), F.lower(F.trim(F.col(\"poi_group\"))).alias(\"group\"))\n",
    "        .filter(F.col(\"p_lat\").isNotNull() & F.col(\"p_lon\").isNotNull() & F.col(\"group\").isNotNull() & (F.col(\"group\") != \"\"))\n",
    "    )\n",
    "\n",
    "    delta_lon = (R_M / (111000.0 * F.cos(F.radians(F.col(\"a.lat\")))))\n",
    "\n",
    "    cand = (\n",
    "        airbnb_scope.alias(\"a\")\n",
    "        .join(\n",
    "            osm_geo.alias(\"p\"),\n",
    "            (F.col(\"p.p_lat\").between(F.col(\"a.lat\") - DELTA_LAT, F.col(\"a.lat\") + DELTA_LAT)) &\n",
    "            (F.col(\"p.p_lon\").between(F.col(\"a.lon\") - delta_lon, F.col(\"a.lon\") + delta_lon)),\n",
    "            \"inner\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    dist = 2 * EARTH_R * F.asin(\n",
    "        F.sqrt(\n",
    "            F.pow(F.sin(F.radians(F.col(\"p.p_lat\") - F.col(\"a.lat\")) / 2), 2) +\n",
    "            F.cos(F.radians(F.col(\"a.lat\"))) * F.cos(F.radians(F.col(\"p.p_lat\"))) *\n",
    "            F.pow(F.sin(F.radians(F.col(\"p.p_lon\") - F.col(\"a.lon\")) / 2), 2)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    per_group = (\n",
    "        cand\n",
    "        .withColumn(\"distance_m\", dist)\n",
    "        .filter(F.col(\"distance_m\") <= R_M)\n",
    "        .groupBy(F.col(\"a.property_id\").alias(\"property_id\"), F.col(\"p.group\").alias(\"group\"))\n",
    "        .agg(F.count(\"*\").cast(\"int\").alias(\"n_places\"))\n",
    "    )\n",
    "\n",
    "    pivoted = per_group.groupBy(\"property_id\").pivot(\"group\").agg(F.first(\"n_places\")).fillna(0)\n",
    "\n",
    "    renamed = pivoted.select(\n",
    "        \"property_id\",\n",
    "        *[F.col(c).alias(f\"env_{_sanitize(c)}\") for c in pivoted.columns if c != \"property_id\"]\n",
    "    )\n",
    "\n",
    "    out = airbnb_scope.join(renamed, \"property_id\", \"left\")\n",
    "\n",
    "    env_cols = [c for c in out.columns if c.startswith(\"env_\")]\n",
    "    out = out.fillna(0, subset=env_cols) if env_cols else out\n",
    "\n",
    "    # Ensure fixed ENV_COLS exist\n",
    "    out = out.select(\"*\", *[F.lit(0).alias(c) for c in ENV_COLS if c not in out.columns]) if \"ENV_COLS\" in globals() else out\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def osm_join_all_continents(spark: SparkSession, airbnb_df: DataFrame, osm_paths: dict) -> DataFrame:\n",
    "    \"\"\"Aggregate OSM environment features across all continents.\"\"\"\n",
    "    parts = [osm_join_one_continent(cont, airbnb_df, spark.read.parquet(p)) for cont, p in osm_paths.items() if cont in continents]\n",
    "\n",
    "    base = (\n",
    "        airbnb_df\n",
    "        .select(\"property_id\", F.col(\"lat\").cast(\"double\").alias(\"lat\"), F.col(\"long\").cast(\"double\").alias(\"lon\"), \"addr_cc\")\n",
    "        .dropDuplicates([\"property_id\"])\n",
    "    )\n",
    "\n",
    "    env_all = parts[0] if parts else base\n",
    "    for p in parts[1:]:\n",
    "        env_all = env_all.unionByName(p, allowMissingColumns=True)\n",
    "\n",
    "    env_cols = [c for c in env_all.columns if c.startswith(\"env_\")]\n",
    "    return env_all.dropDuplicates([\"property_id\"]).fillna(0, subset=env_cols) if env_cols else env_all.dropDuplicates([\"property_id\"])\n",
    "\n",
    "\n",
    "def join_all(out_path: str, cities_path: str = \"dbfs:/vibebnb/data/travel_cities.parquet\", osm_base_dir: str = \"file:/Workspace/Users/nagham.omar@campus.technion.ac.il/VibeBnB/data/osm_pois\") -> DataFrame:\n",
    "    \"\"\"Main pipeline: load, join cities, join OSM env features.\"\"\"\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "    airbnb_df = load_airbnb_data(spark).dropDuplicates([\"property_id\"])\n",
    "\n",
    "    airbnb_with_cities = cities_airbnb_join(spark.read.parquet(cities_path), airbnb_df)\n",
    "\n",
    "    osm_paths = {\n",
    "    \"africa\": f\"{osm_base_dir}/africa_pois_enriched.parquet\",\n",
    "    \"antarctica\": f\"{osm_base_dir}/antarctica_pois_enriched.parquet\",\n",
    "    \"asia\": f\"{osm_base_dir}/asia_pois_enriched.parquet\",\n",
    "    \"australia_oceania\": f\"{osm_base_dir}/australia_oceania_pois_enriched.parquet\",\n",
    "    \"central_america\": f\"{osm_base_dir}/central_america_pois_enriched.parquet\",\n",
    "    \"europe\": f\"{osm_base_dir}/europe_pois_enriched.parquet\",\n",
    "    \"north_america\": f\"{osm_base_dir}/north_america_pois_enriched.parquet\",\n",
    "    \"south_america\": f\"{osm_base_dir}/south_america_pois_enriched.parquet\",\n",
    "}\n",
    "\n",
    "\n",
    "    env_all = osm_join_all_continents(spark, airbnb_df, osm_paths).drop(\"lat\", \"lon\", \"addr_cc\")\n",
    "\n",
    "    final_df = airbnb_with_cities.join(env_all, \"property_id\", \"left\")\n",
    "\n",
    "    env_cols = [c for c in final_df.columns if c.startswith(\"env_\")]\n",
    "    final_df = final_df.fillna(0, subset=env_cols) if env_cols else final_df\n",
    "\n",
    "    # final_df.write.mode(\"overwrite\").partitionBy(\"addr_cc\").parquet(out_path)\n",
    "    return final_df\n",
    "\n",
    "\n",
    "final_df = join_all(out_path=\"dbfs:/vibebnb/data/airbnb_joined_all.parquet\")\n",
    "display(final_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d3733163-e3fe-4a3e-8bd3-8bf2d6bb89e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8a0ee6c2-4cc5-4256-8431-ea448e07db4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# embedding.py\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import (\n",
    "    SQLTransformer, RegexTokenizer, StopWordsRemover,\n",
    "    HashingTF, IDF, VectorAssembler, StandardScaler,\n",
    "    Normalizer, BucketedRandomProjectionLSH\n",
    ")\n",
    "from config import TEXT_COLS, NUM_COLS, ENV_COLS\n",
    "\n",
    "\n",
    "def _present_cols(df: DataFrame, cols):\n",
    "    \"\"\"Return columns that exist in the DataFrame.\"\"\"\n",
    "    return [c for c in cols if c in df.columns]\n",
    "\n",
    "\n",
    "def embedding(df: DataFrame):\n",
    "    \"\"\"Build text + numeric/environment embedding for Airbnb listings.\"\"\"\n",
    "    text_cols = _present_cols(df, TEXT_COLS)\n",
    "    num_cols  = _present_cols(df, NUM_COLS)\n",
    "    env_cols  = _present_cols(df, ENV_COLS)\n",
    "\n",
    "    # combine text columns\n",
    "    text_sql = \" || ' ' || \".join([f\"coalesce({c}, '')\" for c in text_cols])\n",
    "    combine_text = SQLTransformer(statement=f\"SELECT *, lower(trim({text_sql})) AS text_all FROM __THIS__\")\n",
    "\n",
    "    # text processing\n",
    "    tokenizer = RegexTokenizer(inputCol=\"text_all\", outputCol=\"tokens\", pattern=\"\\\\W+\", minTokenLength=2)\n",
    "    stop_rm = StopWordsRemover(inputCol=\"tokens\", outputCol=\"tokens_clean\")\n",
    "    hash_tf = HashingTF(inputCol=\"tokens_clean\", outputCol=\"tf\", numFeatures=1 << 18)\n",
    "    idf = IDF(inputCol=\"tf\", outputCol=\"text_tfidf\")\n",
    "\n",
    "    # numeric + env features\n",
    "    num_env_assembler = VectorAssembler(inputCols=num_cols + env_cols, outputCol=\"num_env_vec\", handleInvalid=\"keep\")\n",
    "    scaler = StandardScaler(inputCol=\"num_env_vec\", outputCol=\"num_env_scaled\", withStd=True, withMean=False)\n",
    "\n",
    "    # final feature vector\n",
    "    final_assembler = VectorAssembler(inputCols=[\"num_env_scaled\", \"text_tfidf\"], outputCol=\"features\", handleInvalid=\"keep\")\n",
    "\n",
    "    pipeline = Pipeline(stages=[combine_text, tokenizer, stop_rm, hash_tf, idf, num_env_assembler, scaler, final_assembler])\n",
    "    model = pipeline.fit(df)\n",
    "\n",
    "    df_emb = model.transform(df).select(\"property_id\", \"features\")\n",
    "\n",
    "    # L2 normalization for similarity search\n",
    "    normalizer = Normalizer(inputCol=\"features\", outputCol=\"features_norm\", p=2.0)\n",
    "    return normalizer.transform(df_emb)\n",
    "\n",
    "\n",
    "def build_lsh(df: DataFrame):\n",
    "    \"\"\"Train LSH model for approximate nearest neighbors.\"\"\"\n",
    "    lsh = BucketedRandomProjectionLSH(\n",
    "        inputCol=\"features_norm\",\n",
    "        outputCol=\"hashes\",\n",
    "        bucketLength=0.5,\n",
    "        numHashTables=3\n",
    "    )\n",
    "    return lsh.fit(df)\n",
    "\n",
    "final_df=embedding(spark.read.parquet(\"dbfs:/vibebnb/data/airbnb_joined_all.parquet\"))\n",
    "model=build_lsh(final_df)\n",
    "# final_df.write.mode(\"overwrite\").partitionBy(\"addr_cc\").parquet\"dbfs:/vibebnb/data/airbnb_embedded.parquet\")\n",
    "#save model\n",
    "# model.save(\"dbfs:/vibebnb/models/lsh_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a0c5fb9-8f75-420e-b6b7-86880ba324df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# scoring.py\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "\n",
    "def airbnb_scores(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"Compute price, property-quality, and host-quality scores (country-normalized).\"\"\"\n",
    "    w = Window.partitionBy(\"addr_cc\")\n",
    "\n",
    "    # price per guest\n",
    "    df1 = df.withColumn(\"PPG\", F.col(\"price_per_night\") / F.col(\"guests\"))\n",
    "    df1 = df1.withColumn(\"PPG_min\", F.min(\"PPG\").over(w)).withColumn(\"PPG_max\", F.max(\"PPG\").over(w))\n",
    "    df1 = df1.withColumn(\n",
    "        \"PPGnorm\",\n",
    "        F.when((F.col(\"PPG_max\") == F.col(\"PPG_min\")) | F.col(\"PPG\").isNull(), F.lit(0.0))\n",
    "         .otherwise((F.col(\"PPG\") - F.col(\"PPG_min\")) / (F.col(\"PPG_max\") - F.col(\"PPG_min\")))\n",
    "    )\n",
    "    df1 = df1.withColumn(\"price_score\", F.lit(1.0) - F.col(\"PPGnorm\"))\n",
    "\n",
    "    # property quality\n",
    "    df1 = df1.withColumn(\n",
    "        \"category_rating_avg\",\n",
    "        (F.col(\"rating_accuracy\") + F.col(\"rating_cleanliness\") + F.col(\"rating_checkin\") +\n",
    "         F.col(\"rating_communication\") + F.col(\"rating_location\") + F.col(\"rating_value\")) / F.lit(6.0)\n",
    "    )\n",
    "    df1 = df1.withColumn(\"rating01\", F.col(\"ratings\") / F.lit(5.0)).withColumn(\"category_rating01\", F.col(\"category_rating_avg\") / F.lit(5.0))\n",
    "    df1 = df1.withColumn(\"rating_score\", F.col(\"rating01\") * F.lit(0.6) + F.col(\"category_rating01\") * F.lit(0.4))\n",
    "\n",
    "    df1 = df1.withColumn(\"number_of_reviews_log\", F.log(F.col(\"property_number_of_reviews\") + F.lit(1.0))).withColumn(\"max_log_cc\", F.max(\"number_of_reviews_log\").over(w))\n",
    "    df1 = df1.withColumn(\n",
    "        \"property_number_of_reviews_weight\",\n",
    "        F.when(F.col(\"max_log_cc\").isNull() | (F.col(\"max_log_cc\") <= 0) | F.col(\"number_of_reviews_log\").isNull(), F.lit(0.0))\n",
    "         .otherwise(F.col(\"number_of_reviews_log\") / F.col(\"max_log_cc\"))\n",
    "    )\n",
    "    df1 = df1.withColumn(\n",
    "        \"weighted_rating_score\",\n",
    "        F.col(\"rating_score\") * F.col(\"property_number_of_reviews_weight\") +\n",
    "        F.lit(0.5) * (F.lit(1.0) - F.col(\"property_number_of_reviews_weight\"))\n",
    "    )\n",
    "    df1 = df1.withColumn(\"property_quality\", F.col(\"weighted_rating_score\") * F.lit(0.8) + F.col(\"is_guest_favorite\") * F.lit(0.2))\n",
    "\n",
    "    # host quality\n",
    "    df1 = df1.withColumn(\"h_i\", F.col(\"host_rating\") / F.lit(5.0)).withColumn(\"e_i\", F.log(F.col(\"host_number_of_reviews\") + F.lit(1.0))).withColumn(\"r_i\", F.col(\"host_response_rate\") / F.lit(100.0)).withColumn(\"T_i\", F.col(\"hosts_year\"))\n",
    "    df1 = df1.withColumn(\"max_e_cc\", F.max(\"e_i\").over(w)).withColumn(\"max_T_cc\", F.max(\"T_i\").over(w))\n",
    "    df1 = df1.withColumn(\n",
    "        \"enorm_i\",\n",
    "        F.when(F.col(\"max_e_cc\").isNull() | (F.col(\"max_e_cc\") <= 0) | F.col(\"e_i\").isNull(), F.lit(0.0))\n",
    "         .otherwise(F.col(\"e_i\") / F.col(\"max_e_cc\"))\n",
    "    )\n",
    "    df1 = df1.withColumn(\"hconf_i\", F.col(\"enorm_i\") * F.col(\"h_i\") + (F.lit(1.0) - F.col(\"enorm_i\")) * F.lit(0.5))\n",
    "    df1 = df1.withColumn(\n",
    "        \"Tnorm_i\",\n",
    "        F.when(F.col(\"max_T_cc\").isNull() | (F.col(\"max_T_cc\") <= 0) | F.col(\"T_i\").isNull(), F.lit(0.0))\n",
    "         .otherwise(F.col(\"T_i\") / F.col(\"max_T_cc\"))\n",
    "    )\n",
    "    df1 = df1.withColumn(\"TenureEffect_i\", F.col(\"Tnorm_i\") * (F.col(\"h_i\") - F.lit(0.5)))\n",
    "    df1 = df1.withColumn(\n",
    "        \"host_quality\",\n",
    "        F.lit(0.6) * F.col(\"hconf_i\") +\n",
    "        F.lit(0.1) * F.col(\"r_i\") +\n",
    "        F.lit(0.1) * F.col(\"is_supperhost\") +\n",
    "        F.lit(0.2) * F.col(\"TenureEffect_i\")\n",
    "    )\n",
    "\n",
    "    return df1\n",
    "\n",
    "\n",
    "def osm_scores(df: DataFrame, env_cols: list[str]) -> DataFrame:\n",
    "    \"\"\"Normalize environment POI counts per country.\"\"\"\n",
    "    w = Window.partitionBy(\"addr_cc\")\n",
    "    out = df\n",
    "    for c in env_cols:\n",
    "        out = out.withColumn(f\"{c}_max_cc\", F.max(F.col(c)).over(w)).withColumn(\n",
    "            f\"{c}_norm\",\n",
    "            F.when(F.col(f\"{c}_max_cc\").isNull() | (F.col(f\"{c}_max_cc\") == 0) | F.col(c).isNull(), F.lit(0.0))\n",
    "             .otherwise(F.col(c) / F.col(f\"{c}_max_cc\"))\n",
    "        )\n",
    "    return out\n",
    "\n",
    "\n",
    "def cities_scores(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"Compute city attributes (monthly avg temps + budget rank).\"\"\"\n",
    "    out = df.withColumn(\"temp_map\", F.from_json(F.col(\"avg_temp_monthly\"), \"map<string, struct<avg:double, max:double, min:double>>\"))\n",
    "    for m in range(1, 13): out = out.withColumn(f\"temp_avg_m{m:02d}\", F.col(\"temp_map\").getItem(F.lit(str(m))).getField(\"avg\"))\n",
    "    rank_map = F.create_map(F.lit(\"Budget\"), F.lit(1), F.lit(\"Mid-range\"), F.lit(2), F.lit(\"Luxury\"), F.lit(3))\n",
    "    out = out.withColumn(\"budget_rank\", rank_map.getItem(F.col(\"budget_level\")))\n",
    "    out = out.withColumn(\"budget_score_raw\", F.when(F.col(\"budget_rank\").isNull(), F.lit(0.0)).otherwise(F.col(\"budget_rank\") / F.lit(3.0)))\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "def scoring_all(df: DataFrame, env_cols: list[str]=ENV_COLS) -> DataFrame:\n",
    "    \"\"\"Run all scoring steps (no user weights applied).\"\"\"\n",
    "    return cities_scores(osm_scores(airbnb_scores(df), env_cols))\n",
    "\n",
    "final_df=scoring_all(spark.read.parquet(\"dbfs:/vibebnb/data/airbnb_embedded.parquet\"))\n",
    "#save \n",
    "# final_df.write.mode(\"overwrite\").parquet(\"dbfs:/vibebnb/data/airbnb_scores.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c7f099d1-4865-43dc-a362-8767c7a82120",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# retrieve.py / rank.py\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import BucketedRandomProjectionLSH\n",
    "\n",
    "def retrieve(target_id, country, df, lsh_model, n=50):\n",
    "    \"\"\"ANN retrieval inside one country using precomputed features_norm.\"\"\"\n",
    "    q = df.filter(F.col(\"property_id\") == target_id).select(\"features_norm\").limit(1).collect()\n",
    "    if not q: return None\n",
    "    q_vec = q[0][\"features_norm\"]\n",
    "    cand = df.filter(F.col(\"addr_cc\") == country)\n",
    "    return lsh_model.approxNearestNeighbors(cand, q_vec, n, distCol=\"l2_dist\")\n",
    "\n",
    "\n",
    "def _normalize_weights(w: dict):\n",
    "    \"\"\"Normalize positive weights to sum to 1.\"\"\"\n",
    "    if not w: return {}\n",
    "    s = sum(v for v in w.values() if v is not None and v > 0)\n",
    "    if s <= 0: return {}\n",
    "    return {k: float(v) / s for k, v in w.items() if v is not None and v > 0}\n",
    "\n",
    "\n",
    "def order(\n",
    "    df: DataFrame,\n",
    "    k: int,\n",
    "    price_w: float = 0.0,\n",
    "    property_w: float = 0.0,\n",
    "    host_w: float = 0.0,\n",
    "    env_weights: dict = None,        # {\"env_food\": 20, \"env_nature\": 10, ...} OR {\"env_food_norm\": 20, ...}\n",
    "    temp_pref: float = None,         # preferred temperature\n",
    "    temp_w: float = 0.0,\n",
    "    travel_month: int = None,        # 1..12\n",
    "    budget_pref: str = None,         # \"Budget\" | \"Mid-range\" | \"Luxury\"\n",
    "    budget_w: float = 0.0,\n",
    "    normalize_all_weights: bool = True,\n",
    "    score_col: str = \"final_score\"\n",
    ") -> DataFrame:\n",
    "    \"\"\"Rank an already-filtered DF using precomputed score components + user weights.\"\"\"\n",
    "    work = df\n",
    "\n",
    "    # weights (optionally normalize everything together)\n",
    "    env_weights = env_weights or {}\n",
    "    all_w = {\"price\": price_w, \"property\": property_w, \"host\": host_w, \"temp\": temp_w, \"budget\": budget_w, **{f\"env::{k}\": v for k, v in env_weights.items()}}\n",
    "    w_norm = _normalize_weights(all_w) if normalize_all_weights else all_w\n",
    "    price_w = w_norm.get(\"price\", 0.0); property_w = w_norm.get(\"property\", 0.0); host_w = w_norm.get(\"host\", 0.0); temp_w = w_norm.get(\"temp\", 0.0); budget_w = w_norm.get(\"budget\", 0.0)\n",
    "\n",
    "    terms = []\n",
    "\n",
    "    # base components from airbnb_scores()\n",
    "    if price_w and \"price_score\" in work.columns: terms.append(F.coalesce(F.col(\"price_score\"), F.lit(0.0)) * F.lit(price_w))\n",
    "    if property_w and \"property_quality\" in work.columns: terms.append(F.coalesce(F.col(\"property_quality\"), F.lit(0.0)) * F.lit(property_w))\n",
    "    if host_w and \"host_quality\" in work.columns: terms.append(F.coalesce(F.col(\"host_quality\"), F.lit(0.0)) * F.lit(host_w))\n",
    "\n",
    "    # env components from osm_scores(): expects <env>_norm columns\n",
    "    if env_weights:\n",
    "        for env_col, raw_w in env_weights.items():\n",
    "            wv = w_norm.get(f\"env::{env_col}\", raw_w) if normalize_all_weights else raw_w\n",
    "            norm_col = env_col if env_col.endswith(\"_norm\") else f\"{env_col}_norm\"\n",
    "            if wv and norm_col in work.columns: terms.append(F.coalesce(F.col(norm_col), F.lit(0.0)) * F.lit(wv))\n",
    "\n",
    "    # temperature component from cities_scores(): uses temp_avg_m01..temp_avg_m12\n",
    "    if temp_w and temp_pref is not None and travel_month is not None:\n",
    "        m = int(travel_month)\n",
    "        if 1 <= m <= 12:\n",
    "            temp_col = f\"temp_avg_m{m:02d}\"\n",
    "            if temp_col in work.columns:\n",
    "                work = work.withColumn(\"temp_score_raw\", F.greatest(F.lit(0.0), F.lit(1.0) - (F.abs(F.col(temp_col) - F.lit(float(temp_pref))) / F.lit(25.0))))\n",
    "                terms.append(F.col(\"temp_score_raw\") * F.lit(temp_w))\n",
    "\n",
    "    # budget component: prefer the score already computed in cities_scores() if present\n",
    "    if budget_w and budget_pref is not None:\n",
    "        rank_map = F.create_map(F.lit(\"Budget\"), F.lit(1), F.lit(\"Mid-range\"), F.lit(2), F.lit(\"Luxury\"), F.lit(3))\n",
    "        work = work.withColumn(\"budget_rank_i\", rank_map.getItem(F.col(\"budget_level\"))).withColumn(\"budget_pref_i\", rank_map.getItem(F.lit(budget_pref)))\n",
    "        work = work.withColumn(\"budget_score_raw_user\", F.when(F.col(\"budget_rank_i\").isNull() | F.col(\"budget_pref_i\").isNull(), F.lit(0.0)).otherwise(F.greatest(F.lit(0.0), F.lit(1.0) - (F.abs(F.col(\"budget_rank_i\") - F.col(\"budget_pref_i\")) / F.lit(2.0)))))\n",
    "        terms.append(F.col(\"budget_score_raw_user\") * F.lit(budget_w))\n",
    "\n",
    "    work = work.withColumn(score_col, F.lit(0.0)) if not terms else work.withColumn(score_col, sum(terms))\n",
    "    return work.orderBy(F.col(score_col).desc()).limit(int(k))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd8d9493-101a-404b-9767-4f6807b16d38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "organize _code",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
