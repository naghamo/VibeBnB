{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d4d6f666-29f2-48c4-b705-af929f7b7c06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## AirBnD Data load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9c422228-9067-4b64-b1d5-6ebacf45a3c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "NOTE:\n",
    "\n",
    "In this notebook im only selecting, casting, exploding cols (with fixed structure and size), adding additional geo data and imputing missing vals.\n",
    "\n",
    "I moved all of this to a proper airbnb_data_loader.py file where we can load the data and use it further for creating embeddings themselves.\n",
    "We can further improve stuff there - like imputation (maybe there is a better technique or optimized method with regressors I didn't know about) or new cols we want to use (i tried to make it modular overall)\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "I've continued to work in airbnb_embeddings_eda.ipynb where i load the data (same logic as in this ipynb), transform it and try to do some initial attempts at embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b649c60f-7d8b-4617-8878-f612e2ecb493",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "storage_account = \"lab94290\"  \n",
    "container = \"airbnb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c059bada-e365-447f-9840-975fa9919472",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sas_token=\"sp=rle&st=2025-12-24T17:37:04Z&se=2026-02-28T01:52:04Z&spr=https&sv=2024-11-04&sr=c&sig=a0lx%2BS6PuS%2FvJ9Tbt4NKdCJHLE9d1Y1D6vpE1WKFQtk%3D\"\n",
    "sas_token = sas_token.lstrip('?')\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{storage_account}.dfs.core.windows.net\", \"SAS\")\n",
    "spark.conf.set(f\"fs.azure.sas.token.provider.type.{storage_account}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.sas.FixedSASTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.sas.fixed.token.{storage_account}.dfs.core.windows.net\", sas_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9f51411-7d4c-4570-964f-ab660c19e19c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "path = f\"abfss://{container}@{storage_account}.dfs.core.windows.net/airbnb_1_12_parquet\"\n",
    "\n",
    "airbnb = spark.read.parquet(path)\n",
    "display(airbnb.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d51c55c-3c46-4283-9d5d-ebf0b4f06d7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "files = dbutils.fs.ls(\"dbfs:/Users\")\n",
    "for f in files:\n",
    "    print(f.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e7e10ff-1813-42ee-9437-3ba300d3e4b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "path = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\n",
    "print(f\"{path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83c72099-417f-4261-b4e5-926b5ecab439",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_folder_size(path):\n",
    "    \"\"\"Recursively get total size of all files in folder\"\"\"\n",
    "    total_size = 0\n",
    "    try:\n",
    "        files = dbutils.fs.ls(path)\n",
    "        for f in files:\n",
    "            if f.isDir():\n",
    "                # Recursively get subdirectory size\n",
    "                total_size += get_folder_size(f.path)\n",
    "            else:\n",
    "                total_size += f.size\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {path}: {e}\")\n",
    "    return total_size\n",
    "\n",
    "total_bytes = get_folder_size(path)\n",
    "size_gb = total_bytes / (1024**3)\n",
    "print(f\"Total size: {size_gb:.2f} GB ({total_bytes:,} bytes)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bea41f2b-d128-4492-8eba-9026367fee96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "airbnb.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a30d91b8-c7bf-49fa-aaba-7dba4f326b60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "airbnb.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40c9414e-5f63-4690-82a0-b42b30ec5407",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "airbnb.count(), airbnb.select(\"property_id\").distinct().count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f0282667-aad8-42c8-bc80-114b43a46fc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Selecting necessary columns and casting them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3635b30-1402-443a-808e-7865939ef410",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{\"final_url\":{\"format\":{\"preset\":\"string-preset-url\",\"locale\":\"en\"}}}},\"syncTimestamp\":1767135400536}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, regexp_replace\n",
    "\n",
    "airbnb_sel = airbnb.select(\n",
    "    \"property_id\",\n",
    "    \"listing_name\",\n",
    "    \"listing_title\",\n",
    "\n",
    "    \"lat\",\n",
    "    \"long\",\n",
    "\n",
    "    \"ratings\",\n",
    "    \"reviews\",\n",
    "    \"property_number_of_reviews\",\n",
    "\n",
    "    \"host_rating\",\n",
    "    \"host_number_of_reviews\",\n",
    "    \"host_response_rate\",\n",
    "    \"hosts_year\",\n",
    "    \"is_supperhost\",\n",
    "    \"is_guest_favorite\",\n",
    "\n",
    "    \"guests\",\n",
    "\n",
    "    \"category\",\n",
    "    \"category_rating\",\n",
    "\n",
    "    \"amenities\",\n",
    "    \"description\",\n",
    "    \"description_items\",\n",
    "    \"details\",\n",
    "    \"arrangement_details\",\n",
    "\n",
    "    \"pricing_details\",\n",
    "    \"total_price\",\n",
    "    \"currency\",\n",
    "\n",
    "    \"availability\",\n",
    "    \"final_url\"\n",
    ")\n",
    "\n",
    "display(airbnb_sel.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eaad40ea-45d1-48d2-b49d-31144c760ced",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, split, trim, lower, size\n",
    "\n",
    "# Removed location logic since we enrich data with reverse geocoder\n",
    "airbnb_cast = airbnb_sel\\\n",
    "    .withColumn(\"lat\", col(\"lat\").cast(\"double\"))\\\n",
    "    .withColumn(\"long\", col(\"long\").cast(\"double\"))\\\n",
    "    .filter(col(\"lat\").isNotNull() & col(\"long\").isNotNull())\\\n",
    "    .withColumn(\"ratings\", col(\"ratings\").cast(\"double\"))\\\n",
    "    .withColumn(\"property_number_of_reviews\", col(\"property_number_of_reviews\").cast(\"int\"))\\\n",
    "    .withColumn(\"host_rating\", col(\"host_rating\").cast(\"double\"))\\\n",
    "    .withColumn(\"host_number_of_reviews\", col(\"host_number_of_reviews\").cast(\"int\"))\\\n",
    "    .withColumn(\"host_response_rate\", col(\"host_response_rate\").cast(\"double\"))\\\n",
    "    .withColumn(\"hosts_year\", col(\"hosts_year\").cast(\"int\"))\\\n",
    "    .withColumn(\"total_price\", col(\"total_price\").cast(\"double\"))\\\n",
    "    .withColumn(\"guests\", col(\"guests\").cast(\"int\"))\\\n",
    "    .withColumn(\"is_supperhost\", (col(\"is_supperhost\") == \"true\").cast(\"int\"))\\\n",
    "    .withColumn(\"is_guest_favorite\", (col(\"is_guest_favorite\") == \"true\").cast(\"int\"))\\\n",
    "    .withColumn(\"is_available\", (col(\"availability\") == \"true\").cast(\"int\"))\\\n",
    "    .dropDuplicates([\"property_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56d22bc5-817c-4238-b4b1-b907757bf090",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType, ArrayType\n",
    "from pyspark.sql.functions import from_json, col, map_from_entries, element_at\n",
    "\n",
    "# Define the specific Schemas for the JSON blobs\n",
    "# We define these manually to ensure correct data types (e.g., handling nulls and doubles)\n",
    "\n",
    "# Schema for pricing_details (simple struct)\n",
    "pricing_schema = StructType([\n",
    "    StructField(\"airbnb_service_fee\", DoubleType()),\n",
    "    StructField(\"cleaning_fee\", DoubleType()),\n",
    "    StructField(\"initial_price_per_night\", DoubleType()),\n",
    "    StructField(\"num_of_nights\", IntegerType()),\n",
    "    StructField(\"price_per_night\", DoubleType()),\n",
    "    StructField(\"price_without_fees\", DoubleType()),\n",
    "    StructField(\"special_offer\", DoubleType()),\n",
    "    StructField(\"taxes\", DoubleType())\n",
    "])\n",
    "\n",
    "# Schema for category_rating (array of structs)\n",
    "# The raw data has \"value\" as a string (\"5.0\"), so we parse as string first\n",
    "category_schema = ArrayType(StructType([\n",
    "    StructField(\"name\", StringType()),\n",
    "    StructField(\"value\", StringType())\n",
    "]))\n",
    "\n",
    "# Apply parsing and extracting\n",
    "airbnb_features = airbnb_cast \\\n",
    "    .withColumn(\"pricing_struct\", from_json(col(\"pricing_details\"), pricing_schema)) \\\n",
    "    .withColumn(\"category_struct\", from_json(col(\"category_rating\"), category_schema)) \\\n",
    "    .select(\n",
    "        \"*\", \n",
    "        # Flatten Pricing: This promotes pricing_struct.cleaning_fee to a top-level column named 'cleaning_fee'\n",
    "        \"pricing_struct.*\" \n",
    "    ) \\\n",
    "    .drop(\"pricing_struct\", \"pricing_details\", \"category_rating\") # Clean up raw cols\n",
    "\n",
    "# Handle Category Ratings\n",
    "# The category_rating is a list like [{\"name\": \"Cleanliness\", \"value\": \"5.0\"}, ...].\n",
    "# We need to turn this into a Map so we can pick out \"Cleanliness\" easily.\n",
    "\n",
    "# Convert Array<Struct> to Map<String, Double>\n",
    "airbnb_features = airbnb_features.withColumn(\n",
    "    \"ratings_map\", \n",
    "    map_from_entries(col(\"category_struct\"))\n",
    ")\n",
    "\n",
    "# Extract specific ratings into their own columns and cast to Double\n",
    "# We use element_at to grab the value by its key name\n",
    "airbnb_features_ext = airbnb_features \\\n",
    "    .withColumn(\"rating_cleanliness\", element_at(col(\"ratings_map\"), \"Cleanliness\").cast(\"double\")) \\\n",
    "    .withColumn(\"rating_accuracy\", element_at(col(\"ratings_map\"), \"Accuracy\").cast(\"double\")) \\\n",
    "    .withColumn(\"rating_checkin\", element_at(col(\"ratings_map\"), \"Check-in\").cast(\"double\")) \\\n",
    "    .withColumn(\"rating_communication\", element_at(col(\"ratings_map\"), \"Communication\").cast(\"double\")) \\\n",
    "    .withColumn(\"rating_location\", element_at(col(\"ratings_map\"), \"Location\").cast(\"double\")) \\\n",
    "    .withColumn(\"rating_value\", element_at(col(\"ratings_map\"), \"Value\").cast(\"double\")) \\\n",
    "    .drop(\"category_struct\", \"ratings_map\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "189679f2-c282-43de-a7fe-6fb9997555ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Adding geo data with revese geocoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72e8d3e9-f0c6-4567-ac60-31c90177458e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install reverse_geocoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c1d7ad7-86f2-4b67-bbfd-625ae68ae9e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf, col\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "import pandas as pd\n",
    "import reverse_geocoder as rg\n",
    "\n",
    "# Initialize\n",
    "print(\"Initializing reverse geocoder...\")\n",
    "rg.search((0, 0))\n",
    "print(\"Ready!\")\n",
    "\n",
    "# Schema for geocoding results - matching POI format\n",
    "geo_schema = StructType([\n",
    "    StructField(\"name\", StringType()),        # city name\n",
    "    StructField(\"cc\", StringType()),          # country code\n",
    "    StructField(\"admin1\", StringType()),      # state/province\n",
    "    StructField(\"admin2\", StringType()),      # county/region\n",
    "])\n",
    "\n",
    "@pandas_udf(geo_schema)\n",
    "def reverse_geocode_udf(lat_series: pd.Series, long_series: pd.Series) -> pd.DataFrame:\n",
    "    \"\"\"Batch reverse geocode lat/long coordinates\"\"\"\n",
    "    # Convert strings to floats, handle nulls\n",
    "    lats = pd.to_numeric(lat_series, errors='coerce')\n",
    "    longs = pd.to_numeric(long_series, errors='coerce')\n",
    "    \n",
    "    # Create coords list, skip invalid ones\n",
    "    coords = []\n",
    "    valid_indices = []\n",
    "    for i, (lat, lon) in enumerate(zip(lats, longs)):\n",
    "        if pd.notna(lat) and pd.notna(lon):\n",
    "            coords.append((lat, lon))\n",
    "            valid_indices.append(i)\n",
    "    \n",
    "    # Geocode valid coords\n",
    "    if coords:\n",
    "        results = rg.search(coords, mode=2)\n",
    "    else:\n",
    "        results = []\n",
    "    \n",
    "    # Build output dataframe\n",
    "    output = []\n",
    "    result_idx = 0\n",
    "    for i in range(len(lat_series)):\n",
    "        if i in valid_indices:\n",
    "            r = results[result_idx]\n",
    "            output.append({\n",
    "                \"name\": r.get(\"name\"),\n",
    "                \"cc\": r.get(\"cc\"),\n",
    "                \"admin1\": r.get(\"admin1\"),\n",
    "                \"admin2\": r.get(\"admin2\"),\n",
    "            })\n",
    "            result_idx += 1\n",
    "        else:\n",
    "            # Invalid coords - return nulls\n",
    "            output.append({\n",
    "                \"name\": None,\n",
    "                \"cc\": None,\n",
    "                \"admin1\": None,\n",
    "                \"admin2\": None,\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d74d90ab-c177-46d9-90a2-e061fa1543ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "import pandas as pd\n",
    "\n",
    "def analyze_missing_values(df: DataFrame, show_all: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Analyze missing values in a PySpark DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : DataFrame\n",
    "        PySpark DataFrame to analyze\n",
    "    show_all : bool, default=False\n",
    "        If True, show all columns. If False, only show columns with missing values.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Summary of missing values with styling\n",
    "    \"\"\"\n",
    "    print(\"Analyzing missing values...\")\n",
    "    \n",
    "    # Calculate missing counts\n",
    "    missing_counts = df.select([\n",
    "        F.count(F.when(F.col(c).isNull(), c)).alias(c)\n",
    "        for c in df.columns\n",
    "    ]).toPandas()\n",
    "    \n",
    "    # Create summary dataframe\n",
    "    total_count = df.count()\n",
    "    missing_df = pd.DataFrame({\n",
    "        'column': missing_counts.columns,\n",
    "        'missing': missing_counts.iloc[0].values,\n",
    "    })\n",
    "    missing_df['missing_pct'] = (missing_df['missing'] / total_count * 100).round(2)\n",
    "    missing_df['present'] = total_count - missing_df['missing']\n",
    "    missing_df['present_pct'] = (missing_df['present'] / total_count * 100).round(2)\n",
    "    \n",
    "    # Sort by missing percentage\n",
    "    missing_df = missing_df.sort_values('missing_pct', ascending=False)\n",
    "    \n",
    "    # Filter if needed\n",
    "    if not show_all:\n",
    "        missing_df = missing_df[missing_df['missing'] > 0]\n",
    "    \n",
    "    missing_df = missing_df.reset_index(drop=True)\n",
    "    \n",
    "    # Print summary\n",
    "    cols_with_missing = (missing_df['missing'] > 0).sum()\n",
    "    print(f\"\\nTotal rows: {total_count:,}\")\n",
    "    print(f\"Total columns: {len(df.columns)}\")\n",
    "    print(f\"Columns with missing values: {cols_with_missing}\\n\")\n",
    "    \n",
    "    # Color function\n",
    "    def color_missing(val):\n",
    "        if val < 1:\n",
    "            return 'background-color: lightgreen'\n",
    "        elif val < 5:\n",
    "            return 'background-color: yellow'\n",
    "        elif val < 30:\n",
    "            return 'background-color: orange'\n",
    "        else:\n",
    "            return 'background-color: red'\n",
    "    \n",
    "    # Style and return\n",
    "    styled_df = missing_df.style.applymap(color_missing, subset=['missing_pct'])\n",
    "    \n",
    "    return styled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad9850e3-57cd-47a6-ac64-7db503bb8765",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Apply geocoding and extract fields with addr_ prefix (matching POI format)\n",
    "airbnb_geo = airbnb_features_ext.withColumn(\n",
    "    \"geo_data\",\n",
    "    reverse_geocode_udf(col(\"lat\"), col(\"long\"))\n",
    ").select(\n",
    "    \"*\",  # Keep all original columns\n",
    "    col(\"geo_data.name\").alias(\"addr_name\"),      # City name\n",
    "    col(\"geo_data.cc\").alias(\"addr_cc\"),          # Country code\n",
    "    col(\"geo_data.admin1\").alias(\"addr_admin1\"),  # State/Province\n",
    "    col(\"geo_data.admin2\").alias(\"addr_admin2\")   # County/Region\n",
    ").drop(\"geo_data\")  # Drop the temporary struct column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ca17f80-6eca-4c6b-a9e2-98cbe3cea4c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "display(analyze_missing_values(airbnb_geo))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5dcb5fe9-6ce6-4ab4-9e50-047e54d1748c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Dropping where perct of missing cols is extremely small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70597a95-051c-4210-8e6d-be04c7ae7ddf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "print(\"Dropping where perct of missing cols is extremely small\")\n",
    "\n",
    "airbnb_filtered = airbnb_geo.filter(\n",
    "    col(\"property_id\").isNotNull() &\n",
    "    col(\"listing_name\").isNotNull() &\n",
    "    col(\"listing_title\").isNotNull() &\n",
    "    col(\"ratings\").isNotNull() &\n",
    "    col(\"is_guest_favorite\").isNotNull() &\n",
    "    col(\"guests\").isNotNull() &\n",
    "    col(\"amenities\").isNotNull() &\n",
    "    col(\"pricing_details\").isNotNull() &\n",
    "    col(\"details\").isNotNull() &\n",
    "    col(\"description_items\").isNotNull()\n",
    ")\n",
    "print(f\"Dropped: {airbnb_features_ext.count() - airbnb_filtered.count():,} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ea78694c-7ac6-4da9-9493-4f05f8f08dcd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Filling rows with appropriate vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c6b0f27-6c1d-4478-a23f-cf44786542b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, coalesce, lit\n",
    "\n",
    "print(\"Filling missing values in numeric/categorical columns...\")\n",
    "\n",
    "airbnb_filled = airbnb_filtered.withColumn(\n",
    "    \"property_number_of_reviews\",\n",
    "    coalesce(col(\"property_number_of_reviews\"), lit(0))\n",
    ").withColumn(\n",
    "    \"host_number_of_reviews\",\n",
    "    coalesce(col(\"host_number_of_reviews\"), lit(0))\n",
    ").withColumn(\n",
    "    \"is_supperhost\",\n",
    "    coalesce(col(\"is_supperhost\"), lit(0))  # Not superhost if missing\n",
    ").withColumn(\n",
    "    \"is_available\",\n",
    "    coalesce(col(\"is_available\"), lit(0))  # Not available if missing\n",
    ").withColumn(\n",
    "    \"cleaning_fee\", \n",
    "    coalesce(col(\"cleaning_fee\"), lit(0.0)) # No fees if missing\n",
    ").withColumn(\n",
    "    \"airbnb_service_fee\", \n",
    "    coalesce(col(\"airbnb_service_fee\"), lit(0.0)) # No fees if missing\n",
    ").withColumn(\n",
    "    \"num_of_nights\", \n",
    "    coalesce(col(\"num_of_nights\"), lit(1)) # base nightly rate\n",
    ").withColumn(\n",
    "    \"taxes\", \n",
    "    coalesce(col(\"taxes\"), lit(0.0)) # No tax if missing\n",
    ").withColumn(\n",
    "    \"special_offer\", \n",
    "    coalesce(col(\"special_offer\"), lit(0.0)) # No special offer if missing\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e3bb9c7-7584-451c-96ad-50975fccc5b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Deriving prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72747969-2b6f-4dd7-8997-ca8a35396c1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_math = airbnb_filled.withColumn(\n",
    "    \"derived_price_per_night\",\n",
    "    (\n",
    "        col(\"total_price\") \n",
    "        - col(\"airbnb_service_fee\") \n",
    "        - col(\"cleaning_fee\") \n",
    "        - col(\"taxes\") \n",
    "        - col(\"special_offer\") # If this is -37, subtracting it adds 37 back to base\n",
    "    ) / col(\"num_of_nights\")\n",
    ")\n",
    "\n",
    "df_reconstructed = df_math.withColumn(\n",
    "    \"final_price_per_night\",\n",
    "    coalesce(\n",
    "        col(\"price_per_night\"),           # Trust explicit price\n",
    "        col(\"initial_price_per_night\"),   # Trust explicit initial price\n",
    "        col(\"price_without_fees\") / col(\"num_of_nights\"), # Trust explicit base price\n",
    "        col(\"derived_price_per_night\"),   # Use our reverse-engineered math\n",
    "        col(\"total_price\") / col(\"num_of_nights\") # Last resort: just divide total by nights\n",
    "    )\n",
    ").withColumn(\n",
    "    \"final_num_of_nights\",\n",
    "    col(\"num_of_nights\") # We use our safe version (defaults to 1)\n",
    ")\n",
    "\n",
    "# Round to 2 decimals and ensure no negatives (e.g., if fees > total due to data error)\n",
    "df_reconstructed = df_reconstructed.withColumn(\n",
    "    \"final_price_per_night\", \n",
    "    when(col(\"final_price_per_night\") < 0, col(\"total_price\")) # Fallback\n",
    "    .otherwise(round(col(\"final_price_per_night\"), 2))\n",
    ").withColumn(\n",
    "    \"final_num_of_nights\", \n",
    "    col(\"final_num_of_nights\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d8a31622-fb3f-4100-a995-9f99aa995f30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Filling text cols appropriately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7104c172-3f4d-4ab2-9868-dd82c1ff97fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, coalesce, lit, when, array\n",
    "\n",
    "airbnb_txt = df_reconstructed.withColumn(\n",
    "    \"description\",\n",
    "    coalesce(\n",
    "        col(\"description\"), \n",
    "        col(\"listing_title\"), \n",
    "        col(\"listing_name\"), \n",
    "        lit(\"No description provided\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a5f44bf-d4ca-4ea2-9d46-9870abf16254",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Converting eur to usd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "170ebc0c-401a-4abb-8f5e-b1a4f9ed41a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "# Convert EUR to USD, keep USD as-is, then drop old columns\n",
    "airbnb_usd = airbnb_txt.withColumn(\n",
    "    \"derived_price_per_night\",\n",
    "    when(col(\"currency\") == \"USD\", col(\"derived_price_per_night\"))\n",
    "    .when(col(\"currency\") == \"EUR\", col(\"derived_price_per_night\") * 1.08) # Using this num to convert EUR to USD\n",
    "    .otherwise(None)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac31b193-6534-43cc-b506-c27a95ce1c00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(analyze_missing_values(airbnb_usd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "785a6dea-220d-4333-8742-8b3911f787a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Imputing price, host/category ratings..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f520eb3-b215-4bac-9695-48b08508174e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg, col, coalesce, lit, broadcast, round\n",
    "\n",
    "# Columns to impute based on Country context\n",
    "country_cols = [\n",
    "    \"host_rating\", \"host_response_rate\", \"hosts_year\",\n",
    "    \"rating_cleanliness\", \"rating_accuracy\", \"rating_checkin\", \n",
    "    \"rating_communication\", \"rating_location\", \"rating_value\", \n",
    "    \"price_per_night\"\n",
    "]\n",
    "\n",
    "# Create expression list for efficient aggregation: [avg(col1), avg(col2)...]\n",
    "agg_exprs = [round(avg(c), 2).alias(f\"avg_{c}\") for c in country_cols]\n",
    "\n",
    "\n",
    "# CALCULATE STATISTICS \n",
    "print(\"Calculating Country & Global Statistics...\")\n",
    "\n",
    "# Country Stats (Group by Country)\n",
    "# One pass to get averages for all columns for every country\n",
    "country_stats = airbnb_usd.groupBy(\"addr_cc\").agg(*agg_exprs)\n",
    "\n",
    "# Global Stats (Scalar Fallbacks)\n",
    "# One pass to get the global average for all columns as fallback\n",
    "global_stats_row = airbnb_usd.agg(*agg_exprs).collect()[0]\n",
    "\n",
    "global_defaults = {}\n",
    "for c in country_cols:\n",
    "    val = global_stats_row[f\"avg_{c}\"]\n",
    "    global_defaults[c] = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca80dc81-386e-404c-a39d-0c2ec25fd94b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(country_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2edbd5bf-a044-4f7b-990c-45ca197d9bbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "global_defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7977d4d0-4326-4c72-9f3b-4d1983c549fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Join the country stats once (Broadcasted for speed)\n",
    "df_impute_ready = airbnb_usd.join(\n",
    "    broadcast(country_stats), \n",
    "    on=\"addr_cc\", \n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "df_imputed = df_impute_ready\n",
    "\n",
    "# Loop through columns and apply the Waterfall Logic\n",
    "for c in country_cols:\n",
    "    df_imputed = df_imputed.withColumn(\n",
    "        c,\n",
    "        coalesce(\n",
    "            col(c),                  # 1. Original Value\n",
    "            col(f\"avg_{c}\"),         # 2. Country Average\n",
    "            lit(global_defaults[c])  # 3. Global Average Fallback\n",
    "        )\n",
    "    ).drop(f\"avg_{c}\") # Drop temp column immediately to keep schema clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6896237-5d2f-4f11-89db-61a177194580",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, regexp_replace\n",
    "\n",
    "airbnb_final = df_imputed.select(\n",
    "    \"property_id\",\n",
    "    \"listing_name\",\n",
    "    \"listing_title\",\n",
    "\n",
    "    \"lat\", \n",
    "    \"long\", \n",
    "    \"addr_name\",            # Enriched City (e.g., \"Broadbeach\")\n",
    "    \"addr_admin1\",          # Enriched State (e.g., \"Queensland\")\n",
    "    \"addr_cc\",              # Enriched Country (e.g., \"AU\")\n",
    "\n",
    "    \"ratings\",\n",
    "    # \"reviews\", # no reviews???\n",
    "    \"property_number_of_reviews\",\n",
    "\n",
    "    \"host_rating\",\n",
    "    \"host_number_of_reviews\",\n",
    "    \"host_response_rate\",\n",
    "    \"hosts_year\",\n",
    "    \"is_supperhost\",\n",
    "    \"is_guest_favorite\",\n",
    "\n",
    "    \"category\",\n",
    "    \"rating_cleanliness\",\n",
    "    \"rating_accuracy\",\n",
    "    \"rating_checkin\",\n",
    "    \"rating_communication\",\n",
    "    \"rating_location\",\n",
    "    \"rating_value\",\n",
    "\n",
    "    \"amenities\",\n",
    "    \"description\",\n",
    "    \"description_items\",\n",
    "    \"details\",\n",
    "    # \"arrangement_details\", # there is details, i think its unnecessary\n",
    "\n",
    "    \"price_per_night\",      # The base unit for comparison (Imputed)\n",
    "    \"num_of_nights\",        # The stay duration base (Imputed)\n",
    "    \"guests\",               # Capacity (Cast to int)\n",
    "\n",
    "    \"is_available\",\n",
    "    \"final_url\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3c0b641-72a9-4aa1-8dff-69203facaa19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(airbnb_final.limit(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "524de2e7-8ab2-4f1b-809e-87a29a52aa11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(analyze_missing_values(airbnb_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6628d85b-c604-4572-b78c-34fd8f1fcfb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import col, coalesce, lit\n",
    "\n",
    "# print(\"Filling missing text columns...\")\n",
    "\n",
    "# airbnb_filled = airbnb_filled.withColumn(\n",
    "#     \"reviews\",\n",
    "#     # Check for both NULL and empty list string \"[]\"\n",
    "#     when(\n",
    "#         (col(\"reviews\").isNull()) | (col(\"reviews\") == \"[]\"), \n",
    "#         lit('[\"No reviews\"]')\n",
    "#     ).otherwise(col(\"reviews\"))\n",
    "# ).withColumn(\n",
    "#     \"description\",\n",
    "#     coalesce(col(\"description\"), lit(\"No description provided\"))\n",
    "# ).withColumn(\n",
    "#     \"arrangement_details\",\n",
    "#     # Check for both NULL and empty list string \"[]\"\n",
    "#     when(\n",
    "#         (col(\"arrangement_details\").isNull()) | (col(\"arrangement_details\") == \"[]\"), \n",
    "#         lit('[{\"name\": \"Arrangement\", \"value\": \"Not specified\"}]')\n",
    "#     ).otherwise(col(\"arrangement_details\"))\n",
    "# )\n",
    "\n",
    "# print(\"Text columns filled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1405cff-6588-4d28-b527-5ed91d96ab92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import col\n",
    "\n",
    "# # List of columns that contain JSON data\n",
    "# json_cols = [\n",
    "#     \"pricing_details\",      # Expecting {}\n",
    "#     \"category_rating\",      # Expecting []\n",
    "#     \"reviews\",              # Expecting []\n",
    "#     \"amenities\",            # Expecting []\n",
    "#     \"arrangement_details\",  # Expecting []\n",
    "#     \"description_items\"     # Expecting []\n",
    "#     \"details\"               # Expecting []\n",
    "# ]\n",
    "\n",
    "# print(\"--- Data Quality Check: Missing or Empty JSON ---\")\n",
    "\n",
    "# for c in json_cols:\n",
    "#     # Check if column exists in DataFrame\n",
    "#     if c in airbnb_filled.columns:\n",
    "#         # Count 1: Actual Nulls\n",
    "#         null_count = airbnb_filled.filter(col(c).isNull()).count()\n",
    "        \n",
    "#         # Count 2: Empty Lists \"[]\"\n",
    "#         empty_list_count = airbnb_filled.filter(col(c) == \"[]\").count()\n",
    "        \n",
    "#         # Count 3: Empty Objects \"{}\"\n",
    "#         empty_obj_count = airbnb_filled.filter(col(c) == \"{}\").count()\n",
    "        \n",
    "#         if null_count > 0 or empty_list_count > 0 or empty_obj_count > 0:\n",
    "#             print(f\"Column: {c}\")\n",
    "#             print(f\"  - NULLs: {null_count}\")\n",
    "#             print(f\"  - Empty Lists '[]': {empty_list_count}\")\n",
    "#             print(f\"  - Empty Objects '{{}}': {empty_obj_count}\")\n",
    "#             print(\"-\" * 20)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "airbnb_cleaning",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
